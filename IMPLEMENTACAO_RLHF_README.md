# üéØ Sistema de Feedback e RLHF - Implementa√ß√£o Completa

## üì¶ O que foi implementado?

Implementei um **sistema completo de coleta de feedback** e preparei seu projeto para **RLHF (Reinforcement Learning from Human Feedback)** com tr√™s n√≠veis de implementa√ß√£o:

### ‚úÖ N√≠vel 1: Sistema de Feedback (IMPLEMENTADO)
- Coleta de thumbs up/down
- Sistema de ratings (1-5 estrelas)
- Coment√°rios dos usu√°rios
- Armazenamento em SQLite
- An√°lise e visualiza√ß√£o de dados
- Interface Gradio atualizada

### üü° N√≠vel 2: DPO - Direct Preference Optimization (PRONTO PARA USO)
- Script de prepara√ß√£o de dados
- Script de treinamento DPO
- Mais simples que RLHF completo
- Requer 500-1000 pares de prefer√™ncia

### üî¥ N√≠vel 3: RLHF Completo com PPO (GUIA INCLU√çDO)
- Guia completo de implementa√ß√£o
- Training de Reward Model
- PPO (Proximal Policy Optimization)
- Para quando tiver 10k+ intera√ß√µes

---

## üìÅ Arquivos Criados

```
src/feedback/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ feedback_collector.py      # Sistema de coleta de feedback
‚îî‚îÄ‚îÄ feedback_analyzer.py        # An√°lise e visualiza√ß√£o

main_with_feedback.py          # Interface Gradio + Feedback
train_dpo.py                   # Script de treinamento DPO
RLHF_GUIDE.md                  # Guia completo de RLHF
```

---

## üöÄ Como Usar

### 1. Execute a interface com sistema de feedback:

```bash
python main_with_feedback.py
```

**Funcionalidades da nova interface:**
- Chat normal com Galileu
- üëç Bot√£o de Thumbs Up
- üëé Bot√£o de Thumbs Down
- ‚≠ê Sistema de rating 1-5
- ‚úçÔ∏è Campo para coment√°rios
- üìä Estat√≠sticas de feedback
- üìà Estat√≠sticas do sistema

### 2. Colete feedback dos usu√°rios:

- Use o chatbot normalmente
- Avalie cada resposta com thumbs up/down
- Opcionalmente, d√™ ratings e coment√°rios detalhados
- Pe√ßa para outras pessoas testarem e avaliarem

**Meta:** Coletar pelo menos 500 feedbacks para treinar com DPO

### 3. Analise o feedback coletado:

```bash
python -c "
from src.feedback.feedback_analyzer import FeedbackAnalyzer
analyzer = FeedbackAnalyzer()
analyzer.generate_report('data/feedback/report.json')
"
```

Ou diretamente:

```bash
python src/feedback/feedback_analyzer.py
```

**O que voc√™ ver√°:**
- Estat√≠sticas gerais
- An√°lise de sentimento
- Qualidade das respostas
- Queries mais comuns
- Problemas identificados
- Recomenda√ß√µes de melhoria

### 4. Quando tiver dados suficientes, treine com DPO:

```bash
python train_dpo.py
```

**Requisitos para DPO:**
- M√≠nimo 50 pares de prefer√™ncia (recomendado 500+)
- GPU com 16GB+ VRAM (ou use Colab)
- 2-4 horas de treinamento

### 5. Use o modelo treinado:

Ap√≥s o treinamento, atualize `config/settings.py`:

```python
# De:
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"

# Para:
MODEL_NAME = "models/galileu_dpo"
```

Depois execute normalmente:

```bash
python main_with_feedback.py
```

---

## üìä Estrutura de Dados

### Feedback armazenado em SQLite:

**Tabela `feedback`:**
- `timestamp`: Data/hora do feedback
- `session_id`: ID da sess√£o
- `query`: Pergunta do usu√°rio
- `response`: Resposta do chatbot
- `rating`: Rating 1-5 (opcional)
- `thumbs_up`: True/False (opcional)
- `comment`: Coment√°rio do usu√°rio (opcional)
- `source_documents`: Documentos usados (JSON)
- `metadata`: Metadados adicionais (JSON)

**Tabela `preference_pairs`:**
- `query`: Pergunta
- `response_chosen`: Resposta melhor
- `response_rejected`: Resposta pior
- `reason`: Raz√£o da escolha
- `metadata`: Metadados

**Localiza√ß√£o do banco:** `data/feedback/feedback.db`

---

## üéØ Fluxo Recomendado

### Fase 1: Coleta de Feedback (Semanas 1-4)
```
1. Execute main_with_feedback.py
2. Use o chatbot normalmente
3. Avalie todas as respostas
4. Compartilhe com amigos/colegas para testar
5. Meta: 500+ feedbacks
```

### Fase 2: An√°lise (Semana 5)
```
1. Execute feedback_analyzer.py
2. Revise estat√≠sticas e problemas
3. Identifique padr√µes
4. Ajuste prompts se necess√°rio
5. Exporte dados para treinamento
```

### Fase 3: Treinamento DPO (Semana 6)
```
1. Verifique se tem 500+ pares
2. Execute train_dpo.py
3. Aguarde treinamento (2-4h)
4. Teste modelo treinado
5. Compare com modelo original
```

### Fase 4: Itera√ß√£o (Cont√≠nuo)
```
1. Continue coletando feedback com novo modelo
2. Analise melhorias
3. Re-treine periodicamente
4. Ciclo de melhoria cont√≠nua
```

---

## üìà Monitoramento

### Verificar estat√≠sticas na interface:

1. Abra `main_with_feedback.py`
2. V√° para a aba "üìä Estat√≠sticas"
3. Clique em "Atualizar Estat√≠sticas de Feedback"

### Via c√≥digo:

```python
from src.feedback.feedback_collector import FeedbackCollector

collector = FeedbackCollector()
stats = collector.get_feedback_stats()
print(stats)
```

### Exportar dados:

```python
# Exportar feedbacks positivos para an√°lise
collector.export_for_training(
    "data/feedback/training_data.jsonl",
    min_rating=4
)

# Exportar pares de prefer√™ncia
collector.export_preference_pairs(
    "data/feedback/preference_pairs.json"
)
```

---

## üÜò Troubleshooting

### "Poucos dados para DPO"
**Solu√ß√£o:** Continue coletando feedback. Voc√™ precisa de pelo menos 50 pares de prefer√™ncia (respostas diferentes para mesmas queries com ratings diferentes).

### "GPU out of memory durante treinamento"
**Solu√ß√µes:**
1. Reduza `batch_size` em `train_dpo.py`
2. Use quantiza√ß√£o 4-bit
3. Use Google Colab com GPU gratuita
4. Considere cloud computing (Lambda Labs, etc)

### "Erro ao importar TRL"
**Solu√ß√£o:**
```bash
pip install trl>=0.7.0
pip install peft>=0.6.0
pip install datasets>=2.14.0
```

### "Modelo treinado n√£o funciona"
**Verifica√ß√µes:**
1. Conferir se `MODEL_NAME` est√° correto em `settings.py`
2. Verificar se modelo foi salvo corretamente
3. Testar com modelo original primeiro
4. Revisar logs de treinamento

---

## üí° Dicas e Boas Pr√°ticas

### Para Coletar Feedback de Qualidade:

1. **Seja consistente:** Avalie todas as respostas, n√£o s√≥ as ruins
2. **Use coment√°rios:** Explique por que deu aquela nota
3. **Teste edge cases:** Perguntas dif√≠ceis ou amb√≠guas
4. **Diversifique:** Fa√ßa perguntas sobre diferentes aspectos de Galileu
5. **Seja honesto:** Avalie objetivamente, n√£o seja gentil demais

### Para Melhor Treinamento DPO:

1. **Diversidade de queries:** Diferentes tipos de perguntas
2. **Pares claros:** Diferen√ßa √≥bvia entre boa e m√° resposta
3. **Volume:** Quanto mais dados, melhor (500-1000 ideal)
4. **Qualidade > Quantidade:** Prefira menos dados de qualidade
5. **Balance:** Mix de queries f√°ceis e dif√≠ceis

### Para Itera√ß√£o Cont√≠nua:

1. **Monitore m√©tricas:** Acompanhe rating m√©dio ao longo do tempo
2. **A/B Testing:** Compare modelo novo vs antigo
3. **Feedback loop:** Use modelo melhorado para gerar mais dados
4. **Documente mudan√ßas:** Anote o que funcionou/n√£o funcionou
5. **Compartilhe resultados:** Mostre melhorias para motivar

---

## üìö Recursos Adicionais

### Documenta√ß√£o:
- [RLHF_GUIDE.md](./RLHF_GUIDE.md) - Guia completo de RLHF
- [TRL Documentation](https://huggingface.co/docs/trl/) - Transformers RL
- [DPO Paper](https://arxiv.org/abs/2305.18290) - Artigo original

### Ferramentas:
- [LangChain](https://python.langchain.com/) - Framework RAG
- [Hugging Face](https://huggingface.co/) - Modelos e datasets
- [Gradio](https://gradio.app/) - Interface web

### Comunidades:
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [LangChain Discord](https://discord.gg/langchain)
- [r/MachineLearning](https://reddit.com/r/MachineLearning)

---

## üé¨ Pr√≥ximos Passos Imediatos

1. ‚úÖ **Instale depend√™ncias do feedback** (j√° inclu√≠das)
   ```bash
   # J√° est√° em requirements.txt
   ```

2. ‚úÖ **Execute a nova interface**
   ```bash
   python main_with_feedback.py
   ```

3. ‚úÖ **Teste o sistema de feedback**
   - Fa√ßa algumas perguntas
   - Avalie as respostas
   - Verifique se dados est√£o sendo salvos

4. ‚úÖ **Compartilhe com outros**
   - Pe√ßa feedback de amigos/colegas
   - Colete pelo menos 50-100 avalia√ß√µes iniciais

5. ‚úÖ **Analise os primeiros dados**
   ```bash
   python src/feedback/feedback_analyzer.py
   ```

6. ‚úÖ **Continue coletando at√© ter 500+**
   - Meta: 500-1000 feedbacks
   - Depois: Treinar com DPO

---

## ü§ù Suporte

Se tiver d√∫vidas ou problemas:

1. Revise `RLHF_GUIDE.md` para detalhes t√©cnicos
2. Verifique os exemplos de c√≥digo nos scripts
3. Consulte a documenta√ß√£o das bibliotecas
4. Abra uma issue no GitHub (se aplic√°vel)

---

## üìù Licen√ßa

Este projeto √© de uso educacional. Siga as licen√ßas dos modelos e bibliotecas utilizados.

---

**Desenvolvido para melhorar continuamente o Chatbot Galileu Galilei! üî≠‚ú®**

Boa sorte com a implementa√ß√£o! üöÄ
