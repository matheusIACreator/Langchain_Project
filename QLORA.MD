# ðŸ”§ Guia de QuantizaÃ§Ã£o (QLoRA)

## O que Ã© QuantizaÃ§Ã£o?

QuantizaÃ§Ã£o reduz a precisÃ£o dos pesos do modelo (de 16-bit para 4-bit), diminuindo drasticamente o uso de memÃ³ria com perda mÃ­nima de qualidade.

## ðŸ“Š ComparaÃ§Ã£o de Uso de MemÃ³ria

| ConfiguraÃ§Ã£o | VRAM NecessÃ¡ria | Qualidade | Velocidade |
|--------------|-----------------|-----------|------------|
| **FP16 (sem quantizaÃ§Ã£o)** | ~16GB | 100% | RÃ¡pida |
| **8-bit (INT8)** | ~8GB | ~98% | MÃ©dia |
| **4-bit (NF4/QLoRA)** | ~4GB | ~95% | Lenta |

### ðŸŽ¯ Para RTX 3050 (4GB VRAM)

**Recomendado:** Use **4-bit quantizaÃ§Ã£o** (QLoRA)
- âœ… Cabe perfeitamente em 4GB
- âœ… Qualidade ainda excelente (~95% do original)
- âš ï¸ Um pouco mais lento na geraÃ§Ã£o

## ðŸš€ Como Usar

### 1. Instalar bitsandbytes
```bash
pip install bitsandbytes>=0.41.0
```

**Nota:** bitsandbytes requer CUDA. NÃ£o funciona apenas com CPU.

### 2. Configurar .env
```bash
USE_QUANTIZATION=True
QUANTIZATION_BITS=4  # 4-bit para 4GB VRAM, 8-bit para 8GB+
```

### 3. Executar normalmente
```bash
python main.py
```

## ðŸ“‹ Tipos de QuantizaÃ§Ã£o

### 4-bit (NF4 - Normal Float 4)

**Melhor para:** GPUs com 4-6GB VRAM (como RTX 3050)

**CaracterÃ­sticas:**
- ðŸŽ¯ Uso de memÃ³ria: ~4GB
- â­ Qualidade: ~95% do modelo original
- âœ… Double Quantization ativa (nested quantization)

### 8-bit (INT8)

**Melhor para:** GPUs com 8-10GB VRAM (como RTX 3060)

**CaracterÃ­sticas:**
- ðŸŽ¯ Uso de memÃ³ria: ~8GB
- â­ Qualidade: ~98% do modelo original

## ðŸ”¬ Detalhes TÃ©cnicos
```python
BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)
```

## ðŸ§ª Benchmarks â€” Llama-3.2-3B-Instruct

| MÃ©trica | FP16 | 8-bit | 4-bit NF4 |
|---------|------|-------|-----------|
| VRAM | 16GB | 8GB | 4GB |
| Tokens/seg | 100 | 85 | 70 |
| Qualidade | 100% | 98% | 95% |

## âš ï¸ Troubleshooting

**"CUDA out of memory"** â†’ Use `QUANTIZATION_BITS=4` ou reduza `max_new_tokens`

**"bitsandbytes not found"**
```bash
pip install bitsandbytes>=0.41.0 --force-reinstall
```

**Windows: "Could not find bitsandbytes CUDA binary"**
```bash
pip uninstall bitsandbytes
pip install bitsandbytes>=0.41.0
```
Se persistir, baixe o wheel manualmente: https://github.com/jllllll/bitsandbytes-windows-webui/releases

## ðŸ“š ReferÃªncias

- [QLoRA Paper](https://arxiv.org/abs/2305.14314)
- [bitsandbytes Documentation](https://github.com/TimDettmers/bitsandbytes)
- [Hugging Face Quantization Guide](https://huggingface.co/docs/transformers/main_classes/quantization)