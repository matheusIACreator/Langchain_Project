# ğŸ”­ Science Chat â€” Historical Figures RAG System

> **Sistema avanÃ§ado de Retrieval-Augmented Generation (RAG) para explorar a histÃ³ria da ciÃªncia atravÃ©s de conversas inteligentes com mÃºltiplas figuras histÃ³ricas.**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3.0-green.svg)](https://www.langchain.com/)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)]()
[![Status](https://img.shields.io/badge/status-v2.1--deployed-brightgreen.svg)]()

**ğŸŒ Demo ao vivo:** [langchain-project-nine.vercel.app](https://langchain-project-nine.vercel.app)  
**âš™ï¸ API:** [shinigami4242557-science-chat-api.hf.space](https://shinigami4242557-science-chat-api.hf.space)

---

## ğŸ“– Sobre o Projeto

Este projeto comeÃ§ou como um chatbot focado em **Galileu Galilei** e evoluiu para um **sistema escalÃ¡vel** capaz de responder perguntas sobre mÃºltiplas figuras histÃ³ricas atravÃ©s de diferentes perÃ­odos da ciÃªncia â€” com frontend moderno, backend em produÃ§Ã£o e pipeline RAG hÃ­brido completo.

### ğŸ¯ O Que o Sistema Faz

- ğŸ’¬ **Conversas Naturais**: FaÃ§a perguntas em portuguÃªs ou inglÃªs
- ğŸ§  **Roteamento Inteligente**: Detecta automaticamente qual figura/perÃ­odo Ã© relevante
- ğŸ” **Busca HÃ­brida**: Combina busca semÃ¢ntica (conceitos) e keyword (nomes/datas)
- ğŸ“š **Multi-PerÃ­odo**: Renascimento, Iluminismo e Era Moderna
- ğŸ¤ **ComparaÃ§Ãµes**: Compare contribuiÃ§Ãµes de diferentes cientistas
- ğŸ’¾ **MemÃ³ria Conversacional**: MantÃ©m contexto da conversa
- âš¡ **Multilingual**: Embeddings que suportam PT/EN/50+ idiomas

---

## ğŸš€ DemonstraÃ§Ã£o

```
VocÃª: "Quando Galileu nasceu?"
Bot: "Galileu Galilei nasceu em 15 de fevereiro de 1564, em Pisa, ItÃ¡lia..."

VocÃª: "E Newton?"
Bot: "Isaac Newton nasceu em 25 de dezembro de 1642, em Woolsthorpe, Inglaterra..."

VocÃª: "Compare as teorias de gravidade de Newton e Einstein"
Bot: "Newton desenvolveu a lei da gravitaÃ§Ã£o universal... Einstein revolucionou esse conceito com a teoria da relatividade geral, propondo que a gravidade Ã© a curvatura do espaÃ§o-tempo..."
```

---

## ğŸ“Š VersÃµes do Projeto

### âœ… v2.1 â€” Deployed (Estado Atual)
**Status**: âœ… Em produÃ§Ã£o

- Sistema multi-figura com 3 cientistas histÃ³ricos
- Embedding multilingual (`paraphrase-multilingual-MiniLM-L12-v2`)
- LLM: Llama 3.2 3B Instruct
- Frontend Next.js deployado na Vercel
- Backend FastAPI deployado no HuggingFace Spaces
- Hybrid retrieval (BM25 + dense + RRF)
- 5 camadas de testes com MRR = 1.0

### ğŸ“Œ v1.0 â€” Single Figure (HistÃ³rico)
**Status**: âœ… EstÃ¡vel

- Chatbot focado exclusivamente em Galileu Galilei
- Vector store com collection Ãºnica
- Interface Gradio

---

## ğŸ—ï¸ Arquitetura do Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Frontend Next.js (Vercel)                â”‚
â”‚     langchain-project-nine.vercel.app        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚ HTTP
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  FastAPI Backend   â”‚ â† HuggingFace Spaces
         â”‚  /chat /health     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Topic Router     â”‚ â† Detecta figura/perÃ­odo/expert
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Dense Search â”‚         â”‚ Sparse Search â”‚
â”‚ (Embeddings) â”‚         â”‚    (BM25)     â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Reciprocal Rank Fusion
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Multi-Collection ChromaDB     â”‚
    â”‚  renaissance/galileo_galilei   â”‚
    â”‚  enlightenment/isaac_newton    â”‚
    â”‚  modern_era/albert_einstein    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Llama 3.2 3B   â”‚ â† CPU (HF Spaces)
        â”‚ + Memory k=5   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Contextualized Answer   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”‘ Componentes Principais

#### 1. **Topic Router** (`src/retrieval/topic_router.py`)
Analisa queries e determina qual expert deve responder (Physics, Biography, Astronomy, etc), quais figuras estÃ£o sendo mencionadas e quais perÃ­odos histÃ³ricos sÃ£o relevantes.

#### 2. **Multi-Collection Vector Store** (`src/vectorstore.py`)
Gerencia mÃºltiplas collections no ChromaDB com estrutura `period/figure`. Suporta lazy loading e busca em uma ou mÃºltiplas collections simultaneamente.

#### 3. **Hybrid Retriever** (`src/retrieval/hybrid_retriever.py`)
Combina Dense Search (embeddings semÃ¢nticos), Sparse Search (BM25) e Reciprocal Rank Fusion para ranking final.

#### 4. **Ingestion Pipeline** (`src/ingestion/pipeline.py`)
Processa biografias automaticamente, divide em chunks de 1000 chars (overlap 200), adiciona metadata rica e organiza por perÃ­odo histÃ³rico.

#### 5. **Conversation Memory** (`src/memory/conversation_memory.py`)
Window buffer das Ãºltimas 5 interaÃ§Ãµes para manter contexto de follow-ups.

---

## ğŸ“š Figuras HistÃ³ricas DisponÃ­veis

| Figura | PerÃ­odo | Anos | Chunks | Fonte |
|--------|---------|------|--------|-------|
| ğŸ”­ Galileu Galilei | Renascimento | 1564â€“1642 | 96 | Wikipedia EN |
| ğŸ Isaac Newton | Iluminismo | 1643â€“1727 | 127 | Wikipedia EN |
| âš›ï¸ Albert Einstein | Era Moderna | 1879â€“1955 | 126 | Wikipedia EN |

**Total**: 3 figuras | 349 chunks | ~243KB de conhecimento histÃ³rico

### ğŸ“… Planejadas para Futuras VersÃµes

**Renascimento**: Leonardo da Vinci, CopÃ©rnico  
**Iluminismo**: Benjamin Franklin, RenÃ© Descartes  
**Era Moderna**: Marie Curie, Charles Darwin, Nikola Tesla, Richard Feynman

---

## ğŸ› ï¸ Tecnologias Utilizadas

**Backend**: FastAPI, LangChain 0.3+, ChromaDB, LangChain-Chroma  
**LLM**: Meta Llama 3.2 3B Instruct  
**Embeddings**: `paraphrase-multilingual-MiniLM-L12-v2` (50+ idiomas)  
**Retrieval**: BM25 (rank-bm25) + Dense embeddings + RRF  
**Frontend**: Next.js 14, TypeScript, shadcn/ui  
**Deploy**: HuggingFace Spaces (backend) + Vercel (frontend)

---

## ğŸ“¦ InstalaÃ§Ã£o Local

### PrÃ©-requisitos

- Python 3.11+
- GPU NVIDIA com CUDA 11.8+ (recomendado) ou CPU
- Token do HuggingFace com acesso ao Llama 3.2
- Node.js 18+ (para o frontend)

### Backend

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/matheusIACreator/Langchain_Project.git
cd Langchain_Project

# 2. Crie ambiente virtual
python -m venv .venv
.venv\Scripts\activate       # Windows
source .venv/bin/activate    # Linux/Mac

# 3. Instale PyTorch com CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 4. Instale dependÃªncias
pip install -r requirements.txt

# 5. Configure o .env
cp .env.example .env
# Edite com seu HF_TOKEN e configuraÃ§Ãµes

# 6. Crie o vector store
python src/ingestion/pipeline.py
python src/vectorstore.py

# 7. Inicie o servidor
uvicorn api:app --reload --port 8000
```

### Frontend

```bash
cd src/frontend/science-chat
npm install
npm run dev
# Acesse http://localhost:3000
```

### VariÃ¡veis de Ambiente

```dotenv
HF_TOKEN=seu_token_aqui
MODEL_NAME=meta-llama/Llama-3.2-3B-Instruct
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
USE_QUANTIZATION=True
QUANTIZATION_BITS=4
NEXT_PUBLIC_API_URL=http://localhost:8000
```

---

## ğŸŒ Deploy

### Backend â€” HuggingFace Spaces

O backend roda em container Docker com CPU bÃ¡sica. No primeiro startup, o vector store Ã© criado automaticamente via `startup.sh`.

```bash
git remote add space https://huggingface.co/spaces/Shinigami4242557/science-chat-api
git push space main
```

VariÃ¡vel necessÃ¡ria no Space (Settings â†’ Secrets):
```
HF_TOKEN=seu_token_aqui
```

### Frontend â€” Vercel

Deploy automÃ¡tico via GitHub. VariÃ¡vel necessÃ¡ria no Vercel:
```
NEXT_PUBLIC_API_URL=https://shinigami4242557-science-chat-api.hf.space
```

---

## ğŸ”Œ API Endpoints

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| GET | `/health` | Health check |
| GET | `/figures` | Lista figuras disponÃ­veis |
| POST | `/chat` | Envia mensagem e recebe resposta |
| DELETE | `/conversation/{session_id}` | Limpa histÃ³rico da sessÃ£o |
| GET | `/stats` | EstatÃ­sticas do sistema |

### Exemplo

```bash
curl -X POST https://shinigami4242557-science-chat-api.hf.space/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quando Galileu nasceu?"}'
```

---

## ğŸ“ Estrutura do Projeto

```
Langchain_Project/
â”œâ”€â”€ api.py                          # FastAPI â€” endpoints principais
â”œâ”€â”€ startup.sh                      # Script de inicializaÃ§Ã£o do container
â”œâ”€â”€ Dockerfile                      # Deploy HF Spaces
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                 # Device, modelo, prompts, geraÃ§Ã£o
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                        # Biografias em texto (Wikipedia)
â”‚       â”œâ”€â”€ renaissance/galileo_galilei.txt
â”‚       â”œâ”€â”€ enlightenment/isaac_newton.txt
â”‚       â””â”€â”€ modern_era/albert_einstein.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/pipeline.py       # Chunking e processamento
â”‚   â”œâ”€â”€ vectorstore.py              # MultiCollectionVectorStore
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ topic_router.py         # Roteamento inteligente
â”‚   â”‚   â””â”€â”€ hybrid_retriever.py     # BM25 + dense + RRF
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â””â”€â”€ rag_chain_multi.py      # Chain Multi-Figura v2.1
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ conversation_memory.py
â”‚   â””â”€â”€ frontend/
â”‚       â””â”€â”€ science-chat/           # Next.js
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_e2e.py                 # 4 camadas de testes
â”‚   â””â”€â”€ test_retrieval_metrics.py   # MRR e NDCG
â””â”€â”€ scripts/
    â””â”€â”€ download_wikipedia_api.py   # IngestÃ£o via Wikipedia API
```

---

## ğŸ§ª Testes e MÃ©tricas

O sistema foi avaliado com 5 camadas de testes antes do deploy:

| Camada | Escopo | Resultado |
|--------|--------|-----------|
| 1 â€” Topic Router | Roteamento sem GPU | âœ… |
| 2 â€” MemÃ³ria | ConversaÃ§Ã£o sem GPU | âœ… |
| 3 â€” Hybrid Retriever | Retrieval sem LLM | âœ… |
| 4 â€” Chain E2E | Pipeline completo | âœ… |
| 5 â€” MRR / NDCG | Qualidade do retrieval | MRR = 1.0 |

```bash
# Testes sem LLM (rÃ¡pido)
pytest tests/test_e2e.py -v -m "not llm"

# MÃ©tricas de retrieval
pytest tests/test_retrieval_metrics.py -v

# Tudo incluindo LLM
pytest tests/test_e2e.py -v
```

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### GPU / CPU

```python
# DetecÃ§Ã£o automÃ¡tica: CUDA > MPS > CPU
# Para forÃ§ar device no .env:
DEVICE=cuda  # ou 'cpu', 'mps'
```

### QuantizaÃ§Ã£o (QLoRA)

| ConfiguraÃ§Ã£o | VRAM | Qualidade | Velocidade |
|--------------|------|-----------|------------|
| FP16 (sem quantizaÃ§Ã£o) | ~6GB | 100% | 100% |
| 8-bit (INT8) | ~4GB | ~98% | ~85% |
| **4-bit (NF4/QLoRA)** | **~2GB** | **~95%** | **~70%** |

### ParÃ¢metros de GeraÃ§Ã£o

```python
# Em config/settings.py:
GENERATION_KWARGS = {
    "max_new_tokens": 1024,
    "temperature": 0.3,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.2,
}
```

---

## ğŸ› Troubleshooting

**CUDA nÃ£o detectado:**
```bash
python -c "import torch; print(torch.cuda.is_available())"
# Se False: reinstalar com --index-url https://download.pytorch.org/whl/cu118
```

**Vector store nÃ£o encontrado:**
```bash
python src/ingestion/pipeline.py
python src/vectorstore.py
```

**Respostas repetitivas:**
```python
# Aumentar repetition_penalty em config/settings.py
"repetition_penalty": 1.3
```

**Respostas em inglÃªs para queries em portuguÃªs:**
```python
# Verificar EMBEDDING_MODEL em settings.py
# Deve ser: paraphrase-multilingual-MiniLM-L12-v2
```

---

## ğŸ—ºï¸ Roadmap

- [x] Sistema RAG bÃ¡sico (Galileu)
- [x] Arquitetura multi-figura
- [x] Hybrid retrieval (BM25 + dense)
- [x] Embedding multilingual
- [x] Frontend Next.js
- [x] Deploy (HF Spaces + Vercel)
- [x] Testes E2E com MRR/NDCG
- [ ] Streaming de respostas
- [ ] Mais figuras histÃ³ricas (Darwin, Curie, Tesla)
- [ ] HistÃ³rico de conversas persistente
- [ ] Backend em GPU para respostas mais rÃ¡pidas

---

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/NovaFigura`)
3. Commit (`git commit -m 'Add: Marie Curie biography'`)
4. Push (`git push origin feature/NovaFigura`)
5. Abra um Pull Request

---

## ğŸ“š ReferÃªncias

- [Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906)
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[SCALING_PLAN.md](docs/SCALING_PLAN.md)**: Arquitetura completa e estratÃ©gia de scaling
- **[QLORA.MD](QLORA.MD)**: Guia de quantizaÃ§Ã£o e otimizaÃ§Ã£o de memÃ³ria

---

## ğŸ“ LicenÃ§a

MIT License â€” veja [LICENSE](LICENSE) para detalhes.

---

## ğŸ‘¨â€ğŸ’» Autor

**Matheus Masago**

- ğŸŒ GitHub: [@matheusIACreator](https://github.com/matheusIACreator)
- ğŸ’¼ LinkedIn: [Matheus Masago](https://www.linkedin.com/in/matheus-masago/)

---

## ğŸ™ Agradecimentos

- **[LangChain](https://www.langchain.com/)** â€” Framework RAG
- **[Hugging Face](https://huggingface.co/)** â€” Modelos e Embeddings
- **[ChromaDB](https://www.trychroma.com/)** â€” Vector Database
- **[Meta AI](https://ai.meta.com/)** â€” Llama Models
- **[Anthropic](https://www.anthropic.com/)** â€” Claude AI (development assistance)
- **[Wikipedia](https://www.wikipedia.org/)** â€” Fonte de dados histÃ³ricos

---

<div align="center">

**âš¡ ConstruÃ­do com Python, LangChain, Next.js e muita â˜•**

[â¬† Voltar ao topo](#-science-chat--historical-figures-rag-system)

</div>