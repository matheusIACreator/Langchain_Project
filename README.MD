# ğŸ”­ Historical Figures RAG System

> **Sistema avanÃ§ado de Retrieval-Augmented Generation (RAG) para explorar a histÃ³ria da ciÃªncia atravÃ©s de conversas inteligentes com mÃºltiplas figuras histÃ³ricas.**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![LangChain](https://img.shields.io/badge/LangChain-0.3.0-green.svg)](https://www.langchain.com/)
[![License](https://img.shields.io/badge/license-Educational-orange.svg)]()
[![Status](https://img.shields.io/badge/status-v2.0--dev-yellow.svg)]()

---

## ğŸ“– Sobre o Projeto

Este projeto comeÃ§ou como um chatbot focado em **Galileu Galilei** e evoluiu para um **sistema escalÃ¡vel** capaz de responder perguntas sobre mÃºltiplas figuras histÃ³ricas atravÃ©s de diferentes perÃ­odos da ciÃªncia.

### ğŸ¯ O Que o Sistema Faz

- ğŸ’¬ **Conversas Naturais**: FaÃ§a perguntas sobre cientistas histÃ³ricos em linguagem natural
- ğŸ§  **Roteamento Inteligente**: Sistema detecta automaticamente qual figura/perÃ­odo Ã© relevante
- ğŸ” **Busca HÃ­brida**: Combina busca semÃ¢ntica (conceitos) e keyword (nomes/datas)
- ğŸ“š **Multi-PerÃ­odo**: Suporta figuras do Renascimento, Iluminismo e Era Moderna
- ğŸ¤ **ComparaÃ§Ãµes**: Compare contribuiÃ§Ãµes de diferentes cientistas
- ğŸ’¾ **MemÃ³ria Conversacional**: MantÃ©m contexto da conversa
- âš¡ **Eficiente**: Roda em GPU de 4GB usando quantizaÃ§Ã£o 4-bit

---

## ğŸš€ DemonstraÃ§Ã£o

```
VocÃª: "Quando Galileu nasceu?"
Bot: "Galileu Galilei nasceu em 15 de fevereiro de 1564, em Pisa, ItÃ¡lia..."

VocÃª: "E Newton?"
Bot: "Isaac Newton nasceu em 25 de dezembro de 1642 (4 de janeiro de 1643 no calendÃ¡rio gregoriano), em Woolsthorpe, Inglaterra..."

VocÃª: "Compare as teorias de gravidade de Newton e Einstein"
Bot: "Newton desenvolveu a lei da gravitaÃ§Ã£o universal, descrevendo a gravidade como uma forÃ§a de atraÃ§Ã£o entre massas... Einstein, por outro lado, revolucionou esse conceito com a teoria da relatividade geral, propondo que a gravidade nÃ£o Ã© uma forÃ§a, mas sim a curvatura do espaÃ§o-tempo causada pela presenÃ§a de massa..."
```

---

## ğŸ“Š VersÃµes do Projeto

### ğŸ“Œ v1.0 - Single Figure (Branch: `main`)
**Status**: âœ… EstÃ¡vel e Funcional

- Chatbot focado exclusivamente em **Galileu Galilei**
- Vector store com collection Ãºnica
- Interface Gradio bÃ¡sica
- MemÃ³ria conversacional

### ğŸš§ v2.0 - Multi-Figure System (Branch: `feature/multi-figure-scaling`)
**Status**: ğŸ—ï¸ Em Desenvolvimento (80% completo)

- Sistema escalÃ¡vel para **mÃºltiplas figuras histÃ³ricas**
- Arquitetura multi-collection (ChromaDB)
- Topic routing inteligente
- Hybrid retrieval (dense + sparse)
- Suporte a 3 perÃ­odos histÃ³ricos

**ğŸ“ˆ Progresso v2.0:**
```
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 80%

âœ… Arquitetura documentada
âœ… Multi-collection vector store
âœ… Topic router
âœ… Hybrid retrieval
âœ… Pipeline de ingestÃ£o
âœ… 3 figuras implementadas
ğŸš§ Interface atualizada
ğŸ“… Mixture of Experts
ğŸ“… MÃ©tricas de avaliaÃ§Ã£o
```

---

## ğŸ—ï¸ Arquitetura do Sistema (v2.0)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User Query                         â”‚
â”‚   "Compare Newton e Einstein"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Topic Router     â”‚ â† Detecta figuras/perÃ­odo/expert
         â”‚  â€¢ Physics Expert  â”‚
         â”‚  â€¢ Biography       â”‚
         â”‚  â€¢ Astronomy       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Dense Search â”‚         â”‚ Sparse Search â”‚
â”‚ (Semantic)   â”‚         â”‚    (BM25)     â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Hybrid Retrieval
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Multi-Collection VectorStore  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ renaissance/            â”‚   â”‚
    â”‚  â”‚   â€¢ galileo_galilei     â”‚   â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
    â”‚  â”‚ enlightenment/          â”‚   â”‚
    â”‚  â”‚   â€¢ isaac_newton        â”‚   â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
    â”‚  â”‚ modern_era/             â”‚   â”‚
    â”‚  â”‚   â€¢ albert_einstein     â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  LLM + Memory  â”‚ â† Llama-3.1-8B (4-bit)
        â”‚  Generate      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Contextualized Answer   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”‘ Componentes Principais

#### 1. **Topic Router** (`src/retrieval/topic_router.py`)
Analisa queries e determina:
- Qual expert deve responder (Physics, Biography, Astronomy, etc)
- Quais figuras estÃ£o sendo mencionadas
- Quais perÃ­odos histÃ³ricos sÃ£o relevantes

#### 2. **Multi-Collection Vector Store** (`src/vectorstore.py`)
- Gerencia mÃºltiplas collections no ChromaDB
- Estrutura: `period/figure` (ex: `renaissance/galileo_galilei`)
- Lazy loading para eficiÃªncia de memÃ³ria
- Busca em uma ou mÃºltiplas collections

#### 3. **Hybrid Retriever** (`src/retrieval/hybrid_retriever.py`)
- **Dense Search**: Busca semÃ¢ntica usando embeddings
- **Sparse Search**: Busca por keywords usando BM25
- **Reciprocal Rank Fusion**: Combina ambos os rankings

#### 4. **Ingestion Pipeline** (`src/ingestion/pipeline.py`)
- Processa biografias automaticamente
- Divide em chunks inteligentes
- Adiciona metadata rica
- Organiza por perÃ­odo histÃ³rico

#### 5. **Conversation Memory** (`src/memory/conversation_memory.py`)
- Buffer window de Ãºltimas 5 interaÃ§Ãµes
- MantÃ©m contexto da conversa
- Permite perguntas de follow-up

---

## ğŸ“š Figuras HistÃ³ricas DisponÃ­veis

### âœ… Renascimento (1400-1600)
| Figura | Status | Chunks | Fonte |
|--------|--------|--------|-------|
| **Galileo Galilei** | âœ… Implementado | 96 | Wikipedia EN |

### âœ… Iluminismo (1650-1800)
| Figura | Status | Chunks | Fonte |
|--------|--------|--------|-------|
| **Isaac Newton** | âœ… Implementado | 127 | Wikipedia EN |

### âœ… Era Moderna (1800-1950)
| Figura | Status | Chunks | Fonte |
|--------|--------|--------|-------|
| **Albert Einstein** | âœ… Implementado | 126 | Wikipedia EN |

### ğŸ“… Planejadas para Futuras VersÃµes

**Renascimento:**
- Leonardo da Vinci
- Michelangelo
- Nicolau CopÃ©rnico

**Iluminismo:**
- Voltaire
- Benjamin Franklin
- RenÃ© Descartes

**Era Moderna:**
- Marie Curie
- Charles Darwin
- Nikola Tesla
- Richard Feynman

**Total Atual**: 3 figuras | 349 chunks | ~242KB de conhecimento histÃ³rico

---

## ğŸ› ï¸ Tecnologias Utilizadas

### Core Stack
- **LLM**: Meta Llama-3.1-8B-Instruct (4-bit quantizaÃ§Ã£o)
- **Vector Database**: ChromaDB (multi-collection)
- **Embeddings**: Sentence Transformers (all-MiniLM-L6-v2)
- **Framework**: LangChain 0.3.0+
- **Interface**: Gradio 4.0+

### Retrieval
- **Dense Search**: Embeddings semÃ¢nticos (384 dimensÃµes)
- **Sparse Search**: BM25 (keyword matching)
- **Fusion**: Reciprocal Rank Fusion

### OtimizaÃ§Ãµes
- **QuantizaÃ§Ã£o**: QLoRA 4-bit (bitsandbytes)
- **Device**: Auto-detect (CUDA > MPS > CPU)
- **Memory**: Lazy loading de collections

---

## ğŸ“¦ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.9+ (recomendado: 3.11 ou 3.12)
- GPU NVIDIA com CUDA 11.8+ (opcional, mas recomendado)
- 4GB+ VRAM (com quantizaÃ§Ã£o) ou 16GB+ (sem quantizaÃ§Ã£o)
- 10GB+ espaÃ§o em disco

### 1. Clone o RepositÃ³rio

```bash
git clone https://github.com/matheusIACreator/Langchain_Project.git
cd Langchain_Project
```

### 2. Escolha a VersÃ£o

**Para v1.0 (Galileu apenas - estÃ¡vel):**
```bash
git checkout main
```

**Para v2.0 (Multi-figura - desenvolvimento):**
```bash
git checkout feature/multi-figure-scaling
```

### 3. Crie Ambiente Virtual

```bash
python -m venv .venv

# Windows
.venv\Scripts\Activate.ps1

# Linux/Mac
source .venv/bin/activate
```

### 4. Instale DependÃªncias

```bash
# DependÃªncias principais
pip install -r requirements.txt

# Para GPU NVIDIA (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 5. Configure VariÃ¡veis de Ambiente

```bash
# Copie o template
cp .env.example .env

# Edite e adicione seu token Hugging Face
# HF_TOKEN=seu_token_aqui
```

**Obtenha seu token**: https://huggingface.co/settings/tokens

### 6. Prepare os Dados

**OpÃ§Ã£o A: Usar dados existentes (v2.0)**
```bash
# JÃ¡ incluÃ­do no branch feature/multi-figure-scaling
# Biografias de Galileu, Newton e Einstein
```

**OpÃ§Ã£o B: Baixar novas figuras (v2.0)**
```bash
python scripts/download_wikipedia_api.py --figures "Charles Darwin,Marie Curie" --language en --output-dir data/raw
```

---

## ğŸ¯ Como Usar

### v1.0 - Chatbot do Galileu

```bash
# 1. Processar documento
python src/document_loader.py

# 2. Criar vector store
python src/vectorstore.py --mode single

# 3. Executar chatbot
python main.py
```

### v2.0 - Sistema Multi-Figura

```bash
# 1. Processar todas as biografias
python src/ingestion/pipeline.py

# 2. Criar multi-collection vector store
python src/vectorstore.py --mode multi

# 3. Testar RAG chain completa
python src/chains/rag_chain_multi.py

# 4. Executar interface (em desenvolvimento)
python main.py
```

### Testes Individuais de Componentes

```bash
# Testar Topic Router
python src/retrieval/topic_router.py

# Testar Hybrid Retriever
python src/retrieval/hybrid_retriever.py

# Baixar novas biografias
python scripts/download_wikipedia_api.py --period enlightenment --language en
```

---

## ğŸ“ Estrutura do Projeto

```
Langchain_Project/
â”œâ”€â”€ .env                        # VariÃ¡veis de ambiente (nÃ£o versionado)
â”œâ”€â”€ .env.example               # Template de configuraÃ§Ã£o
â”œâ”€â”€ .gitignore                 
â”œâ”€â”€ README.md                  # Este arquivo
â”œâ”€â”€ requirements.txt           # DependÃªncias Python
â”œâ”€â”€ main.py                    # Interface principal (Gradio)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py            # ConfiguraÃ§Ãµes gerais do sistema
â”‚   â””â”€â”€ experts_config.py      # ConfiguraÃ§Ã£o de experts (v2.0)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Biografias organizadas por perÃ­odo
â”‚   â”‚   â”œâ”€â”€ renaissance/
â”‚   â”‚   â”‚   â””â”€â”€ galileo_galilei.txt (96 chunks)
â”‚   â”‚   â”œâ”€â”€ enlightenment/
â”‚   â”‚   â”‚   â””â”€â”€ isaac_newton.txt (127 chunks)
â”‚   â”‚   â””â”€â”€ modern_era/
â”‚   â”‚       â””â”€â”€ albert_einstein.txt (126 chunks)
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/             # Dados processados
â”‚   â”‚   â””â”€â”€ chunks/
â”‚   â”‚
â”‚   â””â”€â”€ vectorstore/           # ChromaDB multi-collection
â”‚       â”œâ”€â”€ renaissance/
â”‚       â”‚   â””â”€â”€ galileo_galilei/
â”‚       â”œâ”€â”€ enlightenment/
â”‚       â”‚   â””â”€â”€ isaac_newton/
â”‚       â””â”€â”€ modern_era/
â”‚           â””â”€â”€ albert_einstein/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ document_loader.py    # Processador de PDFs (v1.0)
â”‚   â”œâ”€â”€ vectorstore.py         # Vector store manager (v1.0 + v2.0)
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/             # Sistema de retrieval (v2.0)
â”‚   â”‚   â”œâ”€â”€ topic_router.py           # Roteamento inteligente
â”‚   â”‚   â””â”€â”€ hybrid_retriever.py       # Dense + Sparse search
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/             # Pipeline de dados (v2.0)
â”‚   â”‚   â””â”€â”€ pipeline.py               # Processamento automatizado
â”‚   â”‚
â”‚   â”œâ”€â”€ chains/                # RAG Chains
â”‚   â”‚   â”œâ”€â”€ rag_chain.py              # Single-figure chain (v1.0)
â”‚   â”‚   â””â”€â”€ rag_chain_multi.py        # Multi-figure chain (v2.0)
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ rag_prompts.py            # Templates de prompts
â”‚   â”‚
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ conversation_memory.py    # Sistema de memÃ³ria
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ scripts/                   # Scripts utilitÃ¡rios (v2.0)
â”‚   â””â”€â”€ download_wikipedia_api.py     # Download de biografias
â”‚
â”œâ”€â”€ docs/                      # DocumentaÃ§Ã£o detalhada (v2.0)
â”‚   â”œâ”€â”€ SCALING_PLAN.md               # Arquitetura completa
â”‚   â””â”€â”€ NEXT_STEPS.md                 # Guia de implementaÃ§Ã£o
â”‚
â”œâ”€â”€ notebooks/                 # Jupyter notebooks exploratÃ³rios
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_vectorstore_setup.ipynb
â”‚   â””â”€â”€ 03_rag_testing.ipynb
â”‚
â””â”€â”€ tests/                     # Testes unitÃ¡rios
    â””â”€â”€ test_*.py
```

---

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### GPU / CPU

O sistema detecta automaticamente o melhor device:

```python
# Prioridade: CUDA > MPS (Mac M1/M2) > CPU

# Para forÃ§ar um device especÃ­fico:
# No .env:
DEVICE=cuda  # ou 'cpu', 'mps'
```

### QuantizaÃ§Ã£o (QLoRA)

Para GPUs com 4GB VRAM (como RTX 3050):

```python
# No .env:
USE_QUANTIZATION=True
QUANTIZATION_BITS=4  # 4-bit ou 8-bit
```

**ComparaÃ§Ã£o de uso de memÃ³ria:**

| ConfiguraÃ§Ã£o | VRAM | Qualidade | Velocidade |
|--------------|------|-----------|------------|
| FP16 (sem quantizaÃ§Ã£o) | ~16GB | 100% | 100% |
| 8-bit (INT8) | ~8GB | ~98% | ~85% |
| **4-bit (NF4/QLoRA)** | **~4GB** | **~95%** | **~70%** |

### Chunking

Ajuste o tamanho dos chunks:

```python
# Em config/settings.py:
CHUNK_SIZE = 1000        # Caracteres por chunk
CHUNK_OVERLAP = 200      # Overlap para manter contexto
```

### Retrieval

NÃºmero de documentos recuperados:

```python
# Em config/settings.py:
TOP_K_DOCUMENTS = 4      # Documentos por busca
```

### Memory

Tamanho da janela de memÃ³ria:

```python
# Em src/chains/rag_chain_multi.py:
self.memory = GalileuConversationMemory(
    memory_type="window",
    k=5  # Ãšltimas 5 interaÃ§Ãµes
)
```

---

## ğŸ§ª Exemplos de Uso

### Single-Figure Query (v1.0)

```python
from src.chains.rag_chain import GalileuRAGChain

chain = GalileuRAGChain()

response = chain.chat("Quais foram as principais descobertas de Galileu?")
print(response)
# Output: "Galileu Galilei fez vÃ¡rias descobertas revolucionÃ¡rias..."
```

### Multi-Figure Query (v2.0)

```python
from src.chains.rag_chain_multi import MultiFigureRAGChain

chain = MultiFigureRAGChain()

# Query sobre figura especÃ­fica
response = chain.chat("Quando Newton nasceu?")

# Query comparativa
response = chain.chat("Compare as teorias de gravidade de Newton e Einstein")

# Query com follow-up (usa memÃ³ria)
response = chain.chat("Quem foi Galileu?")
response = chain.chat("Quais foram suas principais invenÃ§Ãµes?")  # Sabe que Ã© sobre Galileu
```

### Download de Novas Figuras

```bash
# Download individual
python scripts/download_wikipedia_api.py \
    --figures "Marie Curie" \
    --language en \
    --output-dir data/raw/modern_era

# Download por perÃ­odo
python scripts/download_wikipedia_api.py \
    --period enlightenment \
    --language en

# Download de mÃºltiplas figuras
python scripts/download_wikipedia_api.py \
    --figures "Charles Darwin,Nikola Tesla,Marie Curie" \
    --language en
```

---

## ğŸ“Š EstatÃ­sticas e MÃ©tricas

### Dados Processados (v2.0)

```
3 perÃ­odos histÃ³ricos
3 figuras cientÃ­ficas
349 chunks totais
~242KB de texto original
~126,000 embeddings (349 chunks Ã— 384 dimensÃµes)
```

### Performance

```
Busca semÃ¢ntica: ~50ms
Busca BM25: ~20ms
Hybrid fusion: ~10ms
LLM generation (4-bit): ~2-3s
Total query end-to-end: ~3-4s
```

### Modelo

```
LLM: Llama-3.1-8B-Instruct
QuantizaÃ§Ã£o: 4-bit NF4
VRAM Usage: ~4GB
Embeddings: all-MiniLM-L6-v2 (384 dims)
Vector DB: ChromaDB (SQLite backend)
```

---

## ğŸš¦ Roadmap

### âœ… Fase 1: FundaÃ§Ã£o (ConcluÃ­da)
- [x] Sistema RAG bÃ¡sico com Galileu
- [x] Arquitetura de scaling documentada
- [x] Topic routing implementado
- [x] Hybrid retrieval implementado
- [x] Multi-collection vector store
- [x] Pipeline de ingestÃ£o
- [x] 3 figuras histÃ³ricas adicionadas

### ğŸš§ Fase 2: IntegraÃ§Ã£o (Em Progresso - 80%)
- [x] RAG Chain multi-figura
- [x] Roteamento integrado
- [x] MemÃ³ria conversacional
- [ ] Interface Gradio atualizada
- [ ] Testes end-to-end completos

### ğŸ“… Fase 3: Mixture of Experts (Planejado)
- [ ] Implementar sistema MoE
- [ ] Fine-tune experts especializados
  - Physics Expert
  - Biography Expert
  - Astronomy Expert
- [ ] Lazy loading de modelos
- [ ] Benchmarks de performance

### ğŸ“… Fase 4: Qualidade e ProduÃ§Ã£o (Planejado)
- [ ] MÃ©tricas de precisÃ£o histÃ³rica
- [ ] ValidaÃ§Ã£o de citaÃ§Ãµes
- [ ] Sistema de feedback do usuÃ¡rio
- [ ] Interface web aprimorada
- [ ] Deploy (Docker + API REST)
- [ ] DocumentaÃ§Ã£o completa de API

### ğŸ“… Fase 5: ExpansÃ£o (Futuro)
- [ ] 10-15 figuras histÃ³ricas
- [ ] Suporte multilingual (PT, ES, FR)
- [ ] Busca temporal (timeline)
- [ ] VisualizaÃ§Ãµes interativas
- [ ] Mobile app

---

## ğŸ§  Conceitos TÃ©cnicos Explicados

### O que Ã© RAG?

**Retrieval-Augmented Generation** combina:
1. **Retrieval**: Busca de informaÃ§Ãµes relevantes em base de conhecimento
2. **Generation**: LLM gera resposta usando as informaÃ§Ãµes encontradas

**Vantagens sobre LLM puro:**
- âœ… InformaÃ§Ãµes atualizadas (nÃ£o limitado ao training cutoff)
- âœ… Cita fontes especÃ­ficas
- âœ… Menos alucinaÃ§Ãµes
- âœ… Controle sobre conhecimento usado

### Dense vs Sparse Retrieval

**Dense (Semantic Search):**
- Usa embeddings vetoriais (384 dimensÃµes)
- Entende significado e contexto
- Bom para: queries conceituais, sinÃ´nimos

**Sparse (BM25):**
- Usa term frequency Ã— inverse document frequency
- Busca palavras exatas
- Bom para: nomes prÃ³prios, datas, termos especÃ­ficos

**Hybrid = Melhor dos dois mundos!**

### Por que Multi-Collection?

**Single Collection** (todos juntos):
```
âŒ Mistura contextos de diferentes figuras
âŒ Busca menos precisa
âŒ DifÃ­cil manter/atualizar
```

**Multi-Collection** (separados):
```
âœ… Contextos isolados
âœ… Busca mais precisa
âœ… FÃ¡cil adicionar/remover figuras
âœ… Permite comparaÃ§Ãµes controladas
```

### Conversation Memory

MantÃ©m contexto para perguntas de follow-up:

```
User: "Quando Galileu nasceu?"
Bot: "Em 1564"
User: "E onde ele morreu?"  â† Sabe que "ele" = Galileu
Bot: "Em Arcetri, perto de FlorenÃ§a"
```

**Window Memory (k=5)**: MantÃ©m Ãºltimas 5 interaÃ§Ãµes para nÃ£o estourar limite de tokens.

---

## ğŸ› Troubleshooting

### Erro: "CUDA out of memory"

**SoluÃ§Ã£o 1**: Usar quantizaÃ§Ã£o 4-bit
```bash
# No .env:
USE_QUANTIZATION=True
QUANTIZATION_BITS=4
```

**SoluÃ§Ã£o 2**: Reduzir batch size
```python
# Em config/settings.py:
GENERATION_KWARGS = {
    "max_new_tokens": 256,  # Reduzir de 512
}
```

### Erro: "Vector store not found"

```bash
# Recriar vector store:
python src/vectorstore.py --mode multi
```

### Erro: "HF_TOKEN not found"

```bash
# 1. Verificar .env existe
cat .env

# 2. Adicionar token
echo "HF_TOKEN=seu_token_aqui" >> .env

# 3. Obter token em: https://huggingface.co/settings/tokens
```

### Performance Lenta

**Causas comuns:**
- CPU mode (sem GPU)
- Modelo sem quantizaÃ§Ã£o
- Muitos documentos sendo recuperados

**SoluÃ§Ãµes:**
```bash
# 1. Verificar device
python -c "import torch; print(torch.cuda.is_available())"

# 2. Ativar quantizaÃ§Ã£o (ver seÃ§Ã£o ConfiguraÃ§Ã£o)

# 3. Reduzir TOP_K_DOCUMENTS em config/settings.py
```

### Respostas GenÃ©ricas / AlucinaÃ§Ãµes

**Causas:**
- Poucos documentos relevantes
- Chunks muito pequenos/grandes
- LLM nÃ£o seguindo contexto

**SoluÃ§Ãµes:**
```python
# 1. Aumentar TOP_K_DOCUMENTS
TOP_K_DOCUMENTS = 6  # Era 4

# 2. Ajustar chunk size
CHUNK_SIZE = 1200  # Era 1000
CHUNK_OVERLAP = 250  # Era 200

# 3. Melhorar prompt (src/prompts/rag_prompts.py)
```

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! 

### Como Contribuir

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/NovaFigura`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: Marie Curie biography'`)
4. Push para a branch (`git push origin feature/NovaFigura`)
5. Abra um Pull Request

### Tipos de ContribuiÃ§Ãµes

- ğŸ“š Adicionar novas figuras histÃ³ricas
- ğŸ› Reportar e corrigir bugs
- ğŸ“ Melhorar documentaÃ§Ã£o
- âœ¨ Propor novas features
- ğŸ§ª Adicionar testes
- ğŸ¨ Melhorar interface

### Guidelines

- CÃ³digo bem documentado (docstrings)
- Seguir estrutura existente
- Testar antes de submeter
- Atualizar README se necessÃ¡rio

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[SCALING_PLAN.md](docs/SCALING_PLAN.md)**: Arquitetura completa e estratÃ©gia de scaling
- **[NEXT_STEPS.md](docs/NEXT_STEPS.md)**: Guia de implementaÃ§Ã£o fase-a-fase
- **[QLORA.MD](QLORA.MD)**: Guia de quantizaÃ§Ã£o e otimizaÃ§Ã£o de memÃ³ria

---

## ğŸ“– ReferÃªncias e Recursos

### Papers
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

### Frameworks e Bibliotecas
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Sentence Transformers](https://www.sbert.net/)

### Tutoriais Relacionados
- [Building a RAG System from Scratch](https://www.langchain.com/tutorials)
- [Vector Databases Explained](https://www.pinecone.io/learn/vector-database/)
- [LLM Quantization Guide](https://huggingface.co/docs/transformers/main_classes/quantization)

---

## ğŸ“ LicenÃ§a

Este projeto Ã© de uso **educacional** e foi desenvolvido para demonstraÃ§Ã£o de tÃ©cnicas de RAG e NLP.

---

## ğŸ‘¨â€ğŸ’» Autor

**Matheus Masago**

- ğŸŒ GitHub: [@matheusIACreator](https://github.com/matheusIACreator)
- ğŸ’¼ LinkedIn: [Matheus Masago](https://www.linkedin.com/in/matheus-masago/)
- ğŸ“§ Email: [contato via LinkedIn]

---

## ğŸ™ Agradecimentos

Este projeto foi desenvolvido com apoio de:

- **[LangChain](https://www.langchain.com/)** - Framework RAG
- **[Hugging Face](https://huggingface.co/)** - Modelos e Embeddings
- **[ChromaDB](https://www.trychroma.com/)** - Vector Database
- **[Meta AI](https://ai.meta.com/)** - Llama Models
- **[Anthropic](https://www.anthropic.com/)** - Claude AI (development assistance)
- **[Wikipedia](https://www.wikipedia.org/)** - Fonte de dados histÃ³ricos

---

## ğŸŒŸ Star History

Se este projeto foi Ãºtil para vocÃª, considere dar uma â­!

[![Star History Chart](https://api.star-history.com/svg?repos=matheusIACreator/Langchain_Project&type=Date)](https://star-history.com/#matheusIACreator/Langchain_Project&Date)

---

## ğŸ“ Contato e Suporte

- **Issues**: [GitHub Issues](https://github.com/matheusIACreator/Langchain_Project/issues)
- **Discussions**: [GitHub Discussions](https://github.com/matheusIACreator/Langchain_Project/discussions)
- **Email**: DisponÃ­vel via LinkedIn

---

<div align="center">

**âš¡ ConstruÃ­do com Python, LangChain, e muita â˜•**

[â¬† Voltar ao topo](#-historical-figures-rag-system)

</div>
