"""
api.py ‚Äî FastAPI Backend para o sistema RAG Multi-Figura v2.0

Endpoints:
  POST /chat                     ‚Äî Enviar mensagem e receber resposta
  DELETE /conversation/{session} ‚Äî Limpar hist√≥rico de uma sess√£o
  GET  /figures                  ‚Äî Listar figuras dispon√≠veis
  GET  /stats                    ‚Äî Estat√≠sticas do sistema
  GET  /health                   ‚Äî Health check

Execu√ß√£o:
  uvicorn api:app --reload --port 8000
"""

import sys
import uuid
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager
sys.stdout.reconfigure(encoding='utf-8')
sys.path.append(str(Path(__file__).resolve().parent))

from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from src.chains.rag_chain_multi import MultiFigureRAGChain
from config.settings import DEBUG, MODEL_NAME, DEVICE


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MODELOS PYDANTIC ‚Äî Request / Response
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000, description="Mensagem do utilizador")
    session_id: Optional[str] = Field(None, description="ID de sess√£o (gerado automaticamente se omitido)")

    model_config = {"json_schema_extra": {"example": {"message": "Quando Newton nasceu?", "session_id": "abc-123"}}}


class RoutingInfo(BaseModel):
    primary_expert: str
    secondary_experts: List[str]
    confidence: float
    routing_reason: str


class SourceDocument(BaseModel):
    content: str
    source_collection: str
    metadata: Dict[str, Any]


class ChatResponse(BaseModel):
    answer: str
    session_id: str
    collections_used: List[str]
    retrieval_mode: str
    routing: RoutingInfo
    source_documents: List[SourceDocument]
    is_greeting: bool
    latency_ms: float


class ConversationDeleteResponse(BaseModel):
    session_id: str
    deleted: bool
    message: str


class FigureInfo(BaseModel):
    key: str
    name: str
    period: str
    collection: str
    years: str
    description: str


class FiguresResponse(BaseModel):
    figures: List[FigureInfo]
    total: int


class SystemStats(BaseModel):
    model: str
    device: str
    retrieval_mode: str
    vectorstore: Dict[str, Any]
    router: Dict[str, Any]
    memory: Dict[str, Any]
    hybrid_retriever: Dict[str, Any]
    active_sessions: int
    uptime_seconds: float


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    vectorstore_ready: bool
    active_sessions: int
    uptime_seconds: float


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ESTADO GLOBAL
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AppState:
    """Estado partilhado da aplica√ß√£o."""

    def __init__(self):
        self.chain: Optional[MultiFigureRAGChain] = None
        # Cada sess√£o tem a sua pr√≥pria inst√¢ncia de mem√≥ria
        # Por simplicidade, usamos a chain global e gerimos sess√µes via dicion√°rio
        self.sessions: Dict[str, Dict] = {}
        self.start_time: float = time.time()

    def get_or_create_session(self, session_id: Optional[str]) -> str:
        """Retorna session_id existente ou cria um novo."""
        if not session_id or session_id not in self.sessions:
            session_id = session_id or str(uuid.uuid4())
            self.sessions[session_id] = {
                "created_at": time.time(),
                "message_count": 0,
            }
        return session_id

    def increment_session(self, session_id: str):
        if session_id in self.sessions:
            self.sessions[session_id]["message_count"] += 1

    def delete_session(self, session_id: str) -> bool:
        if session_id in self.sessions:
            del self.sessions[session_id]
            return True
        return False

    @property
    def uptime(self) -> float:
        return time.time() - self.start_time


state = AppState()


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LIFESPAN ‚Äî Carrega o modelo ao iniciar
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Inicializa a chain ao arrancar e faz cleanup ao parar."""
    print("\nüöÄ Iniciando API ‚Äî carregando modelo...")
    try:
        state.chain = MultiFigureRAGChain()
        print("‚úÖ Chain carregada! API pronta.")
    except Exception as e:
        print(f"‚ùå Erro ao carregar chain: {e}")
        raise

    yield  # API est√° a correr

    print("üõë A encerrar API...")
    state.chain = None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# APP
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

app = FastAPI(
    title="Cientistas Hist√≥ricos ‚Äî RAG API",
    description="Backend RAG Multi-Figura: Galileu ¬∑ Newton ¬∑ Einstein",
    version="2.0.0",
    lifespan=lifespan,
)

# CORS ‚Äî permite qualquer origem em desenvolvimento
# Em produ√ß√£o, substituir ["*"] pela URL do frontend Next.js
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# HELPERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _require_chain():
    """Lan√ßa 503 se a chain n√£o estiver carregada."""
    if state.chain is None:
        raise HTTPException(status_code=503, detail="Modelo ainda n√£o carregado. Tente novamente em instantes.")


def _format_source_documents(docs) -> List[SourceDocument]:
    """Converte documentos LangChain para o modelo Pydantic."""
    result = []
    for doc in docs:
        result.append(SourceDocument(
            content=doc.page_content[:500],  # truncar para n√£o poluir a resposta
            source_collection=doc.metadata.get("source_collection", "unknown"),
            metadata={k: v for k, v in doc.metadata.items() if k != "source_collection"},
        ))
    return result


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENDPOINTS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@app.get("/health", response_model=HealthResponse, tags=["Sistema"])
async def health_check():
    """Verifica se a API est√° operacional."""
    chain_ok = state.chain is not None
    vs_ok = chain_ok and len(state.chain.available_collections) > 0

    return HealthResponse(
        status="ok" if chain_ok else "loading",
        model_loaded=chain_ok,
        vectorstore_ready=vs_ok,
        active_sessions=len(state.sessions),
        uptime_seconds=round(state.uptime, 2),
    )


@app.get("/figures", response_model=FiguresResponse, tags=["Figuras"])
async def list_figures():
    """Lista todas as figuras hist√≥ricas dispon√≠veis no sistema."""
    _require_chain()

    # Metadados est√°ticos das figuras
    FIGURES_METADATA = {
        "galileo_galilei": {
            "name": "Galileu Galilei",
            "period": "Renascimento",
            "collection": "renaissance/galileo_galilei",
            "years": "1564‚Äì1642",
            "description": "Pai da ci√™ncia moderna. Astr√¥nomo, f√≠sico e matem√°tico italiano.",
        },
        "isaac_newton": {
            "name": "Isaac Newton",
            "period": "Iluminismo",
            "collection": "enlightenment/isaac_newton",
            "years": "1643‚Äì1727",
            "description": "Formulou as leis do movimento e da gravita√ß√£o universal.",
        },
        "albert_einstein": {
            "name": "Albert Einstein",
            "period": "Era Moderna",
            "collection": "modern_era/albert_einstein",
            "years": "1879‚Äì1955",
            "description": "Autor da teoria da relatividade. Pr√©mio Nobel de F√≠sica em 1921.",
        },
    }

    available = state.chain.available_collections
    figures = []

    for key, meta in FIGURES_METADATA.items():
        period = meta["collection"].split("/")[0]
        # S√≥ incluir figuras com collection dispon√≠vel
        if any(key in col for cols in available.values() for col in cols):
            figures.append(FigureInfo(key=key, **meta))

    return FiguresResponse(figures=figures, total=len(figures))


@app.post("/chat", response_model=ChatResponse, tags=["Chat"])
async def chat(request: ChatRequest):
    _require_chain()

    session_id = state.get_or_create_session(request.session_id)
    start = time.time()

    try:
        result = state.chain.query(request.message)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erro ao processar query: {str(e)}")

    latency = (time.time() - start) * 1000
    state.increment_session(session_id)

    # Cumprimentos n√£o t√™m routing nem collections
    is_greeting = result.get("is_greeting", False)
    routing_raw = result.get("routing", {})

    routing = RoutingInfo(
        primary_expert=routing_raw.get("primary_expert", "greeting"),
        secondary_experts=routing_raw.get("secondary_experts", []),
        confidence=routing_raw.get("confidence", 1.0),
        routing_reason=routing_raw.get("routing_reason", "Cumprimento detectado"),
    )

    return ChatResponse(
        answer=result["answer"],
        session_id=session_id,
        collections_used=result.get("collections_used", []),
        retrieval_mode=result.get("retrieval_mode", "none"),
        routing=routing,
        source_documents=_format_source_documents(result.get("source_documents", [])),
        is_greeting=is_greeting,
        latency_ms=round(latency, 2),
    )

@app.delete("/conversation/{session_id}", response_model=ConversationDeleteResponse, tags=["Chat"])
async def clear_conversation(session_id: str):
    """
    Limpa o hist√≥rico de mem√≥ria de uma sess√£o.
    √ötil para reiniciar o contexto sem criar uma nova sess√£o.
    """
    _require_chain()

    deleted = state.delete_session(session_id)

    if deleted:
        # Limpar tamb√©m a mem√≥ria interna da chain
        state.chain.clear_conversation()

    return ConversationDeleteResponse(
        session_id=session_id,
        deleted=deleted,
        message="Conversa reiniciada com sucesso." if deleted else "Sess√£o n√£o encontrada.",
    )


@app.get("/stats", response_model=SystemStats, tags=["Sistema"])
async def get_stats():
    """Retorna estat√≠sticas detalhadas do sistema."""
    _require_chain()

    chain_stats = state.chain.get_stats()

    return SystemStats(
        model=chain_stats.get("model", MODEL_NAME),
        device=chain_stats.get("device", DEVICE),
        retrieval_mode=chain_stats.get("retrieval_mode", "hybrid"),
        vectorstore=chain_stats.get("vectorstore", {}),
        router=chain_stats.get("router", {}),
        memory=chain_stats.get("memory", {}),
        hybrid_retriever=chain_stats.get("hybrid_retriever", {}),
        active_sessions=len(state.sessions),
        uptime_seconds=round(state.uptime, 2),
    )