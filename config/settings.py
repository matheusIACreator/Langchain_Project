"""
Configura√ß√µes do Projeto RAG System - Galileu Galilei
Detecta automaticamente GPU/CPU e carrega vari√°veis de ambiente
"""

import os
import torch
from pathlib import Path
from dotenv import load_dotenv
from typing import Literal

# Carrega vari√°veis de ambiente do arquivo .env
load_dotenv()

# ===== PATHS =====
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
CHUNKS_DIR = PROCESSED_DATA_DIR / "chunks"
VECTORSTORE_DIR = DATA_DIR / "vectorstore"

# Cria diret√≥rios se n√£o existirem
for directory in [RAW_DATA_DIR, PROCESSED_DATA_DIR, CHUNKS_DIR, VECTORSTORE_DIR]:
    directory.mkdir(parents=True, exist_ok=True)


# ===== DEVICE CONFIGURATION =====
def get_device() -> str:
    """
    Detecta automaticamente o melhor device dispon√≠vel
    Prioridade: CUDA > MPS (Mac M1/M2) > CPU
    """
    # Verifica se h√° prefer√™ncia manual no .env
    manual_device = os.getenv("DEVICE")
    if manual_device:
        print(f"‚öôÔ∏è  Device manual definido: {manual_device}")
        return manual_device
    
    # Detecta automaticamente
    if torch.cuda.is_available():
        device = "cuda"
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"üöÄ GPU detectada: {gpu_name} ({gpu_memory:.1f}GB)")
    elif torch.backends.mps.is_available():
        device = "mps"
        print("üçé Apple Silicon (MPS) detectado")
    else:
        device = "cpu"
        print("üíª Usando CPU (sem GPU dispon√≠vel)")
    
    return device


DEVICE = get_device()


# ===== HUGGING FACE =====
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN or HF_TOKEN == "your_huggingface_token_here":
    import warnings
    warnings.warn(
        "‚ö†Ô∏è  HF_TOKEN n√£o configurado ‚Äî funcionalidades de LLM indispon√≠veis. "
        "Configure em .env para usar o modelo completo."
    )
    HF_TOKEN = None
# ===== MODEL CONFIGURATION =====
MODEL_NAME = os.getenv("MODEL_NAME", "meta-llama/Llama-3.2-3B-Instruct")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
# Configura√ß√£o de quantiza√ß√£o
USE_QUANTIZATION = os.getenv("USE_QUANTIZATION", "True").lower() == "true"
QUANTIZATION_BITS = int(os.getenv("QUANTIZATION_BITS", "4"))  # 4 ou 8 bits

# Par√¢metros do modelo LLM
if USE_QUANTIZATION and DEVICE == "cuda":
    print(f"üîß Usando quantiza√ß√£o {QUANTIZATION_BITS}-bit (QLoRA)")
    
    # Configura√ß√µes para quantiza√ß√£o
    if QUANTIZATION_BITS == 4:
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",  # Normal Float 4
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,  # Nested quantization
        )
        
        MODEL_KWARGS = {
            "device_map": "auto",
            "quantization_config": bnb_config,
        }
    elif QUANTIZATION_BITS == 8:
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
        )
        
        MODEL_KWARGS = {
            "device_map": "auto",
            "quantization_config": bnb_config,
        }
else:
    print(f"‚öôÔ∏è  Usando modelo sem quantiza√ß√£o (FP16)")
    MODEL_KWARGS = {
        "device_map": "auto" if DEVICE == "cuda" else None,
        "torch_dtype": torch.float16,  # FP16 em CPU e GPU (~6GB RAM)
        "low_cpu_mem_usage": True,     # carrega shard a shard, n√£o tudo de uma vez
    }

# Par√¢metros de gera√ß√£o
GENERATION_KWARGS = {
    "max_new_tokens": 1024,
    "temperature": 0.3,
    "top_p": 0.9,
    "do_sample": True,
    "repetition_penalty": 1.2,
}


# ===== CHROMADB CONFIGURATION =====
CHROMA_PERSIST_DIRECTORY = str(VECTORSTORE_DIR)
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "galileu_collection")


# ===== DOCUMENT PROCESSING =====
# Configura√ß√µes para chunking do PDF
CHUNK_SIZE = 1000  # Tamanho de cada chunk em caracteres
CHUNK_OVERLAP = 200  # Overlap entre chunks para manter contexto


# ===== RETRIEVAL CONFIGURATION =====
# N√∫mero de documentos a recuperar do vectorstore
TOP_K_DOCUMENTS = 4


# ===== MEMORY CONFIGURATION =====
# N√∫mero m√°ximo de mensagens a manter na mem√≥ria
MAX_MEMORY_MESSAGES = 10


# ===== PROMPT TEMPLATES =====
RAG_PROMPT = """Voc√™ √© um assistente especializado em cientistas hist√≥ricos.

INSTRU√á√ïES CR√çTICAS ‚Äî SIGA RIGOROSAMENTE:
1. Responda EXCLUSIVAMENTE com base no CONTEXTO fornecido abaixo
2. N√ÉO use conhecimento externo ao contexto fornecido
3. Responda de forma direta e objetiva em texto corrido
4. N√ÉO adicione se√ß√µes como "Fonte", "Revis√µes", "Resumo", "Conclus√£o"
5. N√ÉO adicione frases como "Caso precise de ajuda adicional..."
6. Se o contexto n√£o contiver a resposta, diga apenas: "N√£o encontrei essa informa√ß√£o nos documentos dispon√≠veis."
7. Seja preciso e cite datas, nomes e eventos espec√≠ficos presentes no contexto
8. Responda em no m√°ximo 3 par√°grafos
9. N√ÉO emita opini√µes pessoais
10. N√ÉO use markdown como **, ## ou ###

**Contexto relevante:**
{context}

**Hist√≥rico da conversa:**
{chat_history}

**Pergunta:** {question}

**Resposta:**"""


# ===== DEBUG MODE =====
DEBUG = os.getenv("DEBUG", "False").lower() == "true"

if DEBUG:
    print("\n" + "="*50)
    print("üîß CONFIGURA√á√ïES DO PROJETO")
    print("="*50)
    print(f"Device: {DEVICE}")
    print(f"Model: {MODEL_NAME}")
    print(f"Embedding Model: {EMBEDDING_MODEL}")
    print(f"Quantization: {USE_QUANTIZATION}")
    if USE_QUANTIZATION and DEVICE == "cuda":
        print(f"Quantization Bits: {QUANTIZATION_BITS}-bit")
    print(f"Chroma Directory: {CHROMA_PERSIST_DIRECTORY}")
    print(f"Collection Name: {COLLECTION_NAME}")
    print(f"Chunk Size: {CHUNK_SIZE}")
    print(f"Top K Documents: {TOP_K_DOCUMENTS}")
    print("="*50 + "\n")