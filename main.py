"""
Main - Interface Gradio v2.0
Sistema RAG Multi-Figura: Galileu, Newton, Einstein
"""

import sys
import os
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent))

import gradio as gr
from typing import List, Tuple

from src.chains.rag_chain_multi import MultiFigureRAGChain
from config.settings import DEBUG, MODEL_NAME, DEVICE

# ===== CONFIGURAÃ‡ÃƒO GLOBAL =====
rag_chain = None

# DiretÃ³rio do vectorstore multi-collection
VECTORSTORE_BASE = Path("data/vectorstore")
EXPECTED_COLLECTIONS = [
    "renaissance/galileo_galilei",
    "enlightenment/isaac_newton",
    "modern_era/albert_einstein",
]

# Figuras e descriÃ§Ãµes para exibir na UI
FIGURES_INFO = {
    "galileo_galilei": {
        "label": "ğŸ”­ Galileu Galilei",
        "period": "Renascimento",
        "years": "1564â€“1642",
        "description": "Pai da ciÃªncia moderna, astrÃ´nomo e fÃ­sico italiano.",
    },
    "isaac_newton": {
        "label": "ğŸ Isaac Newton",
        "period": "Iluminismo",
        "years": "1643â€“1727",
        "description": "Formulou as leis do movimento e da gravitaÃ§Ã£o universal.",
    },
    "albert_einstein": {
        "label": "âš›ï¸ Albert Einstein",
        "period": "Era Moderna",
        "years": "1879â€“1955",
        "description": "Autor da teoria da relatividade e pioneiro da fÃ­sica quÃ¢ntica.",
    },
}

# Exemplos de perguntas organizados por tipo
EXAMPLE_QUESTIONS = [
    # Figura Ãºnica
    "Quando e onde Galileu Galilei nasceu?",
    "Quais foram as principais descobertas de Newton?",
    "O que Einstein contribuiu para a fÃ­sica quÃ¢ntica?",
    # Comparativas
    "Compare as contribuiÃ§Ãµes de Newton e Einstein para a fÃ­sica.",
    "Qual a diferenÃ§a entre a visÃ£o de gravidade de Newton e Einstein?",
    "Como a fÃ­sica evoluiu do Renascimento atÃ© a Era Moderna?",
    # Contextuais
    "O que aconteceu entre Galileu e a Igreja CatÃ³lica?",
    "Como Newton desenvolveu o cÃ¡lculo?",
    "Por que Einstein ganhou o Nobel de FÃ­sica?",
]


# ===== INICIALIZAÃ‡ÃƒO =====

def check_vectorstore() -> Tuple[bool, str]:
    """
    Verifica se o vectorstore multi-collection estÃ¡ disponÃ­vel.

    Returns:
        (ok: bool, mensagem: str)
    """
    if not VECTORSTORE_BASE.exists():
        return False, (
            f"DiretÃ³rio `{VECTORSTORE_BASE}` nÃ£o encontrado.\n\n"
            "Execute:\n"
            "```\n"
            "python src/ingestion/pipeline.py\n"
            "python src/vectorstore.py --mode multi\n"
            "```"
        )

    found = []
    missing = []
    for col in EXPECTED_COLLECTIONS:
        col_path = VECTORSTORE_BASE / col
        if col_path.exists() and any(col_path.iterdir()):
            found.append(col)
        else:
            missing.append(col)

    if not found:
        return False, (
            "Nenhuma collection encontrada em `data/vectorstore/`.\n\n"
            "Execute:\n"
            "```\n"
            "python src/ingestion/pipeline.py\n"
            "python src/vectorstore.py --mode multi\n"
            "```"
        )

    if missing:
        msg = f"Collections disponÃ­veis: {len(found)}/{len(EXPECTED_COLLECTIONS)}\n"
        msg += f"Faltando: {', '.join(missing)}"
        # Parcialmente OK â€” o sistema consegue rodar com menos figuras
        return True, msg

    return True, f"Todas as {len(found)} collections prontas."


def initialize_rag_chain() -> Tuple[bool, str]:
    """
    Inicializa o MultiFigureRAGChain (lazy loading).

    Returns:
        (sucesso: bool, mensagem: str)
    """
    global rag_chain

    if rag_chain is not None:
        return True, "Sistema jÃ¡ inicializado."

    print("\nğŸš€ Inicializando Multi-Figure RAG Chain...")
    try:
        rag_chain = MultiFigureRAGChain()
        print("âœ… Sistema pronto!")
        return True, "Sistema inicializado com sucesso!"
    except Exception as e:
        msg = f"Erro ao inicializar: {str(e)}"
        print(f"âŒ {msg}")
        if DEBUG:
            import traceback
            traceback.print_exc()
        return False, msg


# ===== FUNÃ‡Ã•ES DO CHAT =====

def chat_response(message: str, history: List[Tuple[str, str]]) -> str:
    """
    Processa mensagem e retorna resposta do sistema multi-figura.
    """
    if not message or not message.strip():
        return "Por favor, faÃ§a uma pergunta sobre Galileu, Newton ou Einstein!"

    # Inicializar chain se necessÃ¡rio
    ok, msg = initialize_rag_chain()
    if not ok:
        return f"âŒ Sistema nÃ£o inicializado.\n\n{msg}"

    try:
        response = rag_chain.chat(message)
        return response
    except Exception as e:
        error_msg = f"âŒ Erro ao processar sua pergunta: {str(e)}"
        print(error_msg)
        if DEBUG:
            import traceback
            traceback.print_exc()
        return error_msg


def clear_conversation() -> str:
    """Limpa o histÃ³rico de conversaÃ§Ã£o."""
    if rag_chain is not None:
        rag_chain.clear_conversation()
        return "ğŸ—‘ï¸ Conversa reiniciada! Como posso ajudÃ¡-lo?"
    return "Sistema ainda nÃ£o inicializado."


def get_system_stats() -> str:
    """Retorna estatÃ­sticas detalhadas do sistema v2.0."""
    if rag_chain is None:
        return "âš ï¸ Sistema ainda nÃ£o foi inicializado. Envie uma mensagem primeiro."

    try:
        stats = rag_chain.get_stats()
        memory_stats = stats.get("memory", {})
        vs_stats = stats.get("vectorstore", {})
        router_stats = stats.get("router", {})

        # Montar texto de stats
        lines = [
            "## ğŸ“Š EstatÃ­sticas do Sistema v2.0",
            "",
            f"**Modelo LLM:** `{stats.get('model', 'N/A')}`",
            f"**Device:** `{stats.get('device', 'N/A')}`",
            "",
            "### ğŸ—„ï¸ Vector Store",
            f"- Collections totais: **{vs_stats.get('total_collections', 'N/A')}**",
            f"- Collections carregadas: **{vs_stats.get('collections_loaded', 'N/A')}**",
            f"- Embedding model: `{vs_stats.get('embedding_model', 'N/A')}`",
        ]

        collections = vs_stats.get("collections_list", [])
        if collections:
            lines.append("")
            lines.append("**Collections disponÃ­veis:**")
            for col in sorted(collections):
                lines.append(f"  - `{col}`")

        lines += [
            "",
            "### ğŸ§­ Topic Router",
            f"- Queries roteadas: **{router_stats.get('total_queries', 0)}**",
            f"- Cache hits: **{router_stats.get('cache_hits', 0)}**",
            "",
            "### ğŸ§  MemÃ³ria Conversacional",
            f"- Total de mensagens: **{memory_stats.get('total_messages', 0)}**",
            f"- InteraÃ§Ãµes: **{memory_stats.get('interactions', 0)}**",
            f"- Tipo: `{memory_stats.get('memory_type', 'N/A')}`",
        ]

        if memory_stats.get("memory_type") == "window":
            lines.append(f"- Tamanho da janela: **{memory_stats.get('window_size', 'N/A')}**")

        return "\n".join(lines)

    except Exception as e:
        return f"Erro ao obter estatÃ­sticas: {str(e)}"


def get_routing_info(message: str) -> str:
    """
    Retorna informaÃ§Ã£o de roteamento para uma query (modo debug).
    Ãštil para entender como o Topic Router classificou a pergunta.
    """
    if not message or not message.strip():
        return ""
    if rag_chain is None:
        return ""
    try:
        routing = rag_chain.topic_router.route_query(message)
        expert = routing.get("primary_expert", "N/A")
        confidence = routing.get("confidence", 0)
        reason = routing.get("routing_reason", "")
        secondary = routing.get("secondary_experts", [])

        lines = [f"ğŸ§­ **Expert:** `{expert}` (confianÃ§a: {confidence:.0%})"]
        if secondary:
            lines.append(f"ğŸ”€ **SecundÃ¡rios:** {', '.join(secondary)}")
        if reason:
            lines.append(f"ğŸ’¡ **RazÃ£o:** {reason}")
        return "\n".join(lines)
    except Exception:
        return ""


# ===== INTERFACE GRADIO =====

def create_interface() -> gr.Blocks:
    """Cria a interface Gradio v2.0."""

    custom_css = """
    .container { max-width: 960px; margin: auto; }
    .header {
        text-align: center;
        padding: 20px;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
        color: white;
        border-radius: 12px;
        margin-bottom: 20px;
    }
    .figure-card {
        border: 1px solid #e0e0e0;
        border-radius: 8px;
        padding: 12px;
        background: #fafafa;
    }
    .footer {
        text-align: center;
        margin-top: 30px;
        padding: 16px;
        color: #888;
        font-size: 0.85em;
    }
    """

    with gr.Blocks(title="Cientistas HistÃ³ricos â€” RAG Multi-Figura", css=custom_css) as demo:

        # â”€â”€ Header â”€â”€
        gr.Markdown("""
        <div class="header">
            <h1>ğŸ›ï¸ Cientistas HistÃ³ricos</h1>
            <p>Sistema RAG Multi-Figura Â· Galileu Â· Newton Â· Einstein</p>
        </div>
        """)

        # â”€â”€ Cards das figuras â”€â”€
        with gr.Row():
            for fig_key, info in FIGURES_INFO.items():
                with gr.Column():
                    gr.Markdown(f"""
**{info['label']}**
*{info['period']} Â· {info['years']}*

{info['description']}
                    """)

        gr.Markdown("---")

        # â”€â”€ Sobre o sistema â”€â”€
        with gr.Accordion("â„¹ï¸ Sobre este sistema", open=False):
            gr.Markdown(f"""
**VersÃ£o:** 2.0 Â· Multi-Figura

**Como funciona:**
1. Sua pergunta Ã© analisada pelo **Topic Router**, que identifica quais figuras e experts sÃ£o relevantes.
2. O **Hybrid Retriever** busca os trechos mais relevantes nas collections ChromaDB (busca semÃ¢ntica + BM25).
3. O **LLM** (`{MODEL_NAME}`) gera uma resposta contextualizada com base nos documentos recuperados.
4. A **memÃ³ria conversacional** mantÃ©m o contexto ao longo da conversa.

**VocÃª pode:**
- Perguntar sobre uma figura especÃ­fica: *"Quem foi Newton?"*
- Fazer perguntas comparativas: *"Compare Newton e Einstein"*
- Explorar perÃ­odos: *"Como a fÃ­sica evoluiu do Renascimento Ã  Era Moderna?"*
- Fazer perguntas de acompanhamento â€” o sistema mantÃ©m o contexto!
            """)

        # â”€â”€ Chat principal â”€â”€
        chatbot = gr.ChatInterface(
            fn=chat_response,
            examples=EXAMPLE_QUESTIONS,
            title="",
            description="ğŸ’¬ Pergunte sobre Galileu Galilei, Isaac Newton ou Albert Einstein",
            retry_btn="ğŸ”„ Tentar novamente",
            undo_btn="â†©ï¸ Desfazer",
            clear_btn="ğŸ—‘ï¸ Limpar conversa",
        )

        gr.Markdown("---")

        # â”€â”€ Painel inferior: Stats + Debug â”€â”€
        with gr.Row():
            with gr.Column(scale=1):
                with gr.Accordion("ğŸ“Š EstatÃ­sticas do Sistema", open=False):
                    stats_display = gr.Markdown("*Clique em 'Atualizar' para ver as estatÃ­sticas.*")
                    stats_btn = gr.Button("ğŸ”„ Atualizar EstatÃ­sticas", size="sm")
                    stats_btn.click(fn=get_system_stats, inputs=None, outputs=stats_display)

            with gr.Column(scale=1):
                with gr.Accordion("ğŸ§­ Debug: Topic Router", open=False):
                    gr.Markdown("*Veja como o sistema classificou sua Ãºltima pergunta.*")
                    debug_input = gr.Textbox(
                        placeholder="Cole aqui sua pergunta para ver o roteamento...",
                        label="Pergunta",
                        lines=2,
                    )
                    debug_output = gr.Markdown()
                    debug_btn = gr.Button("ğŸ” Analisar Roteamento", size="sm")
                    debug_btn.click(fn=get_routing_info, inputs=debug_input, outputs=debug_output)

        # â”€â”€ Footer â”€â”€
        gr.Markdown("""
        <div class="footer">
            <p><strong>Desenvolvido por Matheus Masago</strong> Â· RAG System v2.0 com LangChain</p>
            <p>ChromaDB Â· all-MiniLM-L6-v2 Â· BM25 Â· Reciprocal Rank Fusion</p>
        </div>
        """)

    return demo


# ===== FUNÃ‡ÃƒO PRINCIPAL =====

def main():
    print("\n" + "=" * 60)
    print("ğŸš€ INICIANDO SISTEMA RAG MULTI-FIGURA v2.0")
    print("=" * 60 + "\n")

    # Verificar vectorstore antes de subir a interface
    vs_ok, vs_msg = check_vectorstore()
    if not vs_ok:
        print(f"âŒ ERRO: Vectorstore nÃ£o encontrado ou vazio.\n")
        print(vs_msg)
        print("\nğŸ’¡ Execute os comandos acima e tente novamente.")
        return

    print(f"âœ… Vectorstore: {vs_msg}")
    print(f"ğŸ¤– Modelo: {MODEL_NAME}")
    print(f"âš™ï¸  Device: {DEVICE}")
    print("\nğŸ“Œ A chain serÃ¡ inicializada na primeira mensagem (lazy loading).")
    print("\nğŸŒ Iniciando interface web...")
    print("ğŸ’¡ Use Ctrl+C para encerrar\n")

    demo = create_interface()

    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        quiet=False,
        inbrowser=True,
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Encerrando aplicaÃ§Ã£o... AtÃ© logo!")
    except Exception as e:
        print(f"\nâŒ Erro fatal: {str(e)}")
        if DEBUG:
            import traceback
            traceback.print_exc()
        sys.exit(1)