"""
Camada 5 â€” MÃ©tricas de AvaliaÃ§Ã£o do Retrieval
MRR (Mean Reciprocal Rank) e NDCG (Normalized Discounted Cumulative Gain)

Avalia a qualidade do Hybrid Retriever com um conjunto de queries
com ground truth definido manualmente.

ExecuÃ§Ã£o:
    pytest tests/test_retrieval_metrics.py -v
    pytest tests/test_retrieval_metrics.py -v --tb=short
"""

import math
import sys
import pytest
from pathlib import Path
from typing import List, Dict, Tuple

sys.path.append(str(Path(__file__).resolve().parent.parent))


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GROUND TRUTH
# Cada entrada define:
#   - query: pergunta de avaliaÃ§Ã£o
#   - collection: collection esperada
#   - relevant_keywords: termos que DEVEM aparecer nos docs recuperados
#   - figure: figura esperada nos metadados
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GROUND_TRUTH: List[Dict] = [
    # â”€â”€ Galileu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "query": "Quando e onde Galileu nasceu?",
        "collection": "renaissance/galileo_galilei",
        "figure": "galileo_galilei",
        "relevant_keywords": ["1564", "pisa", "galileo", "february"],
    },
    {
        "query": "Galileu e o telescÃ³pio",
        "collection": "renaissance/galileo_galilei",
        "figure": "galileo_galilei",
        "relevant_keywords": ["telescope", "galileo", "observations", "lens"],
    },
    {
        "query": "Galileu e a InquisiÃ§Ã£o",
        "collection": "renaissance/galileo_galilei",
        "figure": "galileo_galilei",
        "relevant_keywords": ["heliocentrism", "pope", "urban", "advocate", "dialogue", "geocentric"],
    },
    {
        "query": "Galileu e as luas de JÃºpiter",
        "collection": "renaissance/galileo_galilei",
        "figure": "galileo_galilei",
        "relevant_keywords": ["jupiter", "moons", "satellites", "galileo"],
    },
    {
        "query": "queda livre galileu experimentos fÃ­sica",
        "collection": "renaissance/galileo_galilei",
        "figure": "galileo_galilei",
        "relevant_keywords": ["fall", "motion", "physics", "experiment", "velocity"],
    },
    # â”€â”€ Newton â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "query": "Quando Newton nasceu?",
        "collection": "enlightenment/isaac_newton",
        "figure": "isaac_newton",
        "relevant_keywords": ["1643", "newton", "woolsthorpe", "cambridge"],
    },
    {
        "query": "Newton e a lei da gravitaÃ§Ã£o universal",
        "collection": "enlightenment/isaac_newton",
        "figure": "isaac_newton",
        "relevant_keywords": ["gravitation", "gravity", "newton", "law"],
    },
    {
        "query": "Newton e o cÃ¡lculo matemÃ¡tico",
        "collection": "enlightenment/isaac_newton",
        "figure": "isaac_newton",
        "relevant_keywords": ["calculus", "leibniz", "mathematics", "fluxions"],
    },
    {
        "query": "Principia Mathematica Newton",
        "collection": "enlightenment/isaac_newton",
        "figure": "isaac_newton",
        "relevant_keywords": ["principia", "philosophiae", "naturalis", "mathematica"],
    },
    {
        "query": "Newton Ã³ptica luz prisma cores",
        "collection": "enlightenment/isaac_newton",
        "figure": "isaac_newton",
        "relevant_keywords": ["optics", "light", "prism", "colour", "spectrum"],
    },
    # â”€â”€ Einstein â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
        "query": "Quando Einstein nasceu?",
        "collection": "modern_era/albert_einstein",
        "figure": "albert_einstein",
        "relevant_keywords": ["1879", "einstein", "ulm", "germany", "born"],
    },
    {
        "query": "Einstein e a teoria da relatividade",
        "collection": "modern_era/albert_einstein",
        "figure": "albert_einstein",
        "relevant_keywords": ["relativity", "einstein", "space", "time", "special"],
    },
    {
        "query": "Einstein Nobel de FÃ­sica",
        "collection": "modern_era/albert_einstein",
        "figure": "albert_einstein",
        "relevant_keywords": ["nobel", "einstein", "1921", "prize", "physics"],
    },
    {
        "query": "E=mcÂ² energia massa Einstein",
        "collection": "modern_era/albert_einstein",
        "figure": "albert_einstein",
        "relevant_keywords": ["energy", "mass", "equivalence", "light", "speed"],
    },
    {
        "query": "Einstein efeito fotoelÃ©trico quantum",
        "collection": "modern_era/albert_einstein",
        "figure": "albert_einstein",
        "relevant_keywords": ["photoelectric", "quantum", "photon", "light", "effect"],
    },
]
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNÃ‡Ã•ES DE MÃ‰TRICAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def is_relevant(doc, ground_truth: Dict) -> bool:
    """
    Determina se um documento Ã© relevante para uma query.

    Um documento Ã© considerado relevante se:
    1. Pertence Ã  collection esperada (via metadados), E
    2. ContÃ©m pelo menos um keyword relevante no conteÃºdo
    """
    content = doc.page_content.lower()
    source = doc.metadata.get("source_collection", "").lower()

    # Verificar collection
    expected_collection = ground_truth["collection"].lower()
    collection_match = expected_collection in source or source in expected_collection

    # Verificar keywords
    keywords = ground_truth["relevant_keywords"]
    keyword_match = any(kw.lower() in content for kw in keywords)

    return collection_match and keyword_match


def reciprocal_rank(docs: List, ground_truth: Dict) -> float:
    """
    Calcula o Reciprocal Rank para uma query.

    RR = 1 / posiÃ§Ã£o_do_primeiro_doc_relevante
    Se nenhum doc Ã© relevante â†’ RR = 0
    """
    for rank, doc in enumerate(docs, start=1):
        if is_relevant(doc, ground_truth):
            return 1.0 / rank
    return 0.0


def dcg(docs: List, ground_truth: Dict, k: int) -> float:
    """
    Discounted Cumulative Gain atÃ© posiÃ§Ã£o k.

    DCG@k = Î£ rel_i / log2(i + 1)
    rel_i = 1 se doc na posiÃ§Ã£o i Ã© relevante, 0 caso contrÃ¡rio
    """
    score = 0.0
    for i, doc in enumerate(docs[:k], start=1):
        relevance = 1.0 if is_relevant(doc, ground_truth) else 0.0
        score += relevance / math.log2(i + 1)
    return score


def ideal_dcg(ground_truth: Dict, k: int) -> float:
    """
    IDCG â€” DCG ideal assumindo que todos os docs relevantes
    aparecem nas primeiras posiÃ§Ãµes.

    Para o nosso caso binÃ¡rio (relevante/nÃ£o relevante),
    o IDCG@k = Î£ 1/log2(i+1) para i=1..min(num_relevantes, k)
    Assumimos que hÃ¡ pelo menos 1 documento relevante.
    """
    # Estimativa conservadora: assumimos que existem pelo menos
    # k documentos relevantes no corpus (jÃ¡ que temos 96-127 por collection)
    num_relevant = k  # assume que todos os k docs poderiam ser relevantes
    return sum(1.0 / math.log2(i + 1) for i in range(1, num_relevant + 1))


def ndcg(docs: List, ground_truth: Dict, k: int) -> float:
    """
    Normalized DCG atÃ© posiÃ§Ã£o k.

    NDCG@k = DCG@k / IDCG@k
    Retorna valor entre 0 e 1.
    """
    idcg = ideal_dcg(ground_truth, k)
    if idcg == 0:
        return 0.0
    return dcg(docs, ground_truth, k) / idcg


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FIXTURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@pytest.fixture(scope="module")
def hybrid_retriever():
    """Inicializa o retriever hÃ­brido uma vez para todos os testes."""
    from src.vectorstore import MultiCollectionVectorStore
    from src.retrieval.hybrid_retriever import MultiCollectionHybridRetriever

    vs_dir = Path("data/vectorstore")
    if not vs_dir.exists() or not any(vs_dir.rglob("*.sqlite3")):
        pytest.skip("Vectorstore nÃ£o encontrado. Execute: python src/vectorstore.py --mode multi")

    vs = MultiCollectionVectorStore()
    retriever = MultiCollectionHybridRetriever(vs)
    return retriever


@pytest.fixture(scope="module")
def topic_router():
    """Inicializa o Topic Router uma vez para todos os testes."""
    from src.retrieval.topic_router import TopicRouter
    return TopicRouter()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CAMADA 5 â€” TESTES DE MÃ‰TRICAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestRetrievalMetrics:
    """
    Avalia a qualidade do Hybrid Retriever com mÃ©tricas MRR e NDCG.

    Os testes individuais validam cada query do ground truth.
    O teste de sumÃ¡rio calcula as mÃ©dias globais e imprime o relatÃ³rio.
    """

    K = 8  # nÃºmero de documentos recuperados por query

    # â”€â”€ Testes por query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    @pytest.mark.parametrize("gt", GROUND_TRUTH, ids=[g["query"][:40] for g in GROUND_TRUTH])
    def test_retrieval_retorna_docs_relevantes(self, hybrid_retriever, gt):
        """Cada query deve retornar pelo menos 1 documento relevante."""
        period, figure = gt["collection"].split("/")
        docs = hybrid_retriever.retrieve(
            query=gt["query"],
            collections=[(period, figure)],
            k_per_collection=self.K,
            k_final=self.K,
        )
        assert len(docs) > 0, f"Nenhum documento retornado para: {gt['query']}"

        has_relevant = any(is_relevant(doc, gt) for doc in docs)
        assert has_relevant, (
            f"Nenhum doc relevante para: '{gt['query']}'\n"
            f"  Esperado: {gt['collection']} + keywords {gt['relevant_keywords']}\n"
            f"  Recebido: {[doc.metadata.get('source_collection') for doc in docs]}"
        )

    @pytest.mark.parametrize("gt", GROUND_TRUTH, ids=[g["query"][:40] for g in GROUND_TRUTH])
    def test_mrr_por_query_aceitavel(self, hybrid_retriever, gt):
        """RR de cada query deve ser â‰¥ 0.25 (doc relevante nas top 4 posiÃ§Ãµes)."""
        period, figure = gt["collection"].split("/")
        docs = hybrid_retriever.retrieve(
            query=gt["query"],
            collections=[(period, figure)],
            k_per_collection=self.K,
            k_final=self.K,
        )
        rr = reciprocal_rank(docs, gt)
        assert rr >= 0.25, (
            f"RR baixo para '{gt['query']}': {rr:.3f}\n"
            f"  Doc relevante nÃ£o encontrado nas top 4 posiÃ§Ãµes."
        )

    @pytest.mark.parametrize("gt", GROUND_TRUTH, ids=[g["query"][:40] for g in GROUND_TRUTH])
    def test_ndcg_por_query_aceitavel(self, hybrid_retriever, gt):
        """NDCG@8 de cada query deve ser â‰¥ 0.3."""
        period, figure = gt["collection"].split("/")
        docs = hybrid_retriever.retrieve(
            query=gt["query"],
            collections=[(period, figure)],
            k_per_collection=self.K,
            k_final=self.K,
        )
        score = ndcg(docs, gt, k=self.K)
        assert score >= 0.3, (
            f"NDCG@{self.K} baixo para '{gt['query']}': {score:.3f}"
        )

    # â”€â”€ SumÃ¡rio global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def test_mrr_global_aceitavel(self, hybrid_retriever):
        """MRR mÃ©dio sobre todas as queries deve ser â‰¥ 0.5."""
        rrs = []
        for gt in GROUND_TRUTH:
            period, figure = gt["collection"].split("/")
            docs = hybrid_retriever.retrieve(
                query=gt["query"],
                collections=[(period, figure)],
                k_per_collection=self.K,
                k_final=self.K,
            )
            rrs.append(reciprocal_rank(docs, gt))

        mrr = sum(rrs) / len(rrs)

        print(f"\n{'='*60}")
        print(f"ðŸ“Š MÃ‰TRICAS DE RETRIEVAL â€” RAG Multi-Figura v2.0")
        print(f"{'='*60}")
        print(f"  Queries avaliadas : {len(GROUND_TRUTH)}")
        print(f"  K (top-k)         : {self.K}")
        print(f"  MRR               : {mrr:.4f}  (threshold â‰¥ 0.50)")

        # Por figura
        for figure_key in ["galileo_galilei", "isaac_newton", "albert_einstein"]:
            figure_gts = [g for g in GROUND_TRUTH if g["figure"] == figure_key]
            figure_rrs = []
            for gt in figure_gts:
                period, figure = gt["collection"].split("/")
                docs = hybrid_retriever.retrieve(
                    query=gt["query"],
                    collections=[(period, figure)],
                    k_per_collection=self.K,
                    k_final=self.K,
                )
                figure_rrs.append(reciprocal_rank(docs, gt))
            fig_mrr = sum(figure_rrs) / len(figure_rrs) if figure_rrs else 0
            label = figure_key.replace("_", " ").title()
            print(f"    {label:<20}: MRR = {fig_mrr:.4f}")

        print(f"{'='*60}\n")

        assert mrr >= 0.5, f"MRR global insuficiente: {mrr:.4f} (mÃ­nimo: 0.50)"

    def test_ndcg_global_aceitavel(self, hybrid_retriever):
        """NDCG@8 mÃ©dio sobre todas as queries deve ser â‰¥ 0.5."""
        scores = []
        for gt in GROUND_TRUTH:
            period, figure = gt["collection"].split("/")
            docs = hybrid_retriever.retrieve(
                query=gt["query"],
                collections=[(period, figure)],
                k_per_collection=self.K,
                k_final=self.K,
            )
            scores.append(ndcg(docs, gt, k=self.K))

        mean_ndcg = sum(scores) / len(scores)

        print(f"\n  NDCG@{self.K}           : {mean_ndcg:.4f}  (threshold â‰¥ 0.50)")

        # Por figura
        for figure_key in ["galileo_galilei", "isaac_newton", "albert_einstein"]:
            figure_gts = [g for g in GROUND_TRUTH if g["figure"] == figure_key]
            figure_scores = []
            for gt in figure_gts:
                period, figure = gt["collection"].split("/")
                docs = hybrid_retriever.retrieve(
                    query=gt["query"],
                    collections=[(period, figure)],
                    k_per_collection=self.K,
                    k_final=self.K,
                )
                figure_scores.append(ndcg(docs, gt, k=self.K))
            fig_ndcg = sum(figure_scores) / len(figure_scores) if figure_scores else 0
            label = figure_key.replace("_", " ").title()
            print(f"    {label:<20}: NDCG@{self.K} = {fig_ndcg:.4f}")

        assert mean_ndcg >= 0.5, f"NDCG@{self.K} global insuficiente: {mean_ndcg:.4f} (mÃ­nimo: 0.50)"

    def test_colecao_correta_nos_resultados(self, hybrid_retriever):
        """
        Para cada query, todos os docs devem vir da collection correcta.
        Valida o isolamento entre figuras.
        """
        violacoes = []
        for gt in GROUND_TRUTH:
            period, figure = gt["collection"].split("/")
            docs = hybrid_retriever.retrieve(
                query=gt["query"],
                collections=[(period, figure)],
                k_per_collection=self.K,
                k_final=self.K,
            )
            for doc in docs:
                source = doc.metadata.get("source_collection", "")
                if gt["collection"] not in source and source not in gt["collection"]:
                    violacoes.append(
                        f"Query '{gt['query'][:30]}': doc de '{source}' em vez de '{gt['collection']}'"
                    )

        assert len(violacoes) == 0, (
            f"Isolamento violado em {len(violacoes)} casos:\n" +
            "\n".join(f"  - {v}" for v in violacoes[:5])
        )